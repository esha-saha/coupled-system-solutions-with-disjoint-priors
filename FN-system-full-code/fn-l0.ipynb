{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4b115",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "# from pyDOE2 import lhs\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494b595f",
   "metadata": {},
   "source": [
    "### Load the data taken from [Chen et. al. (2021)](https://github.com/isds-neu/EQDiscovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735314c",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 100.0\n",
    "N = 200\n",
    "dx = L / N\n",
    "dt = 0.0005\n",
    "steps = 120000\n",
    "print('time interval is [0,',dt*steps,']')\n",
    "\n",
    "gamma_u = 1\n",
    "gamma_v = 100\n",
    "alpha = 0.01\n",
    "beta = 0.25\n",
    "\n",
    "data_u_clean = np.load('/home/esaha/links/projects/def-wanghao/esaha/fhn_codes_data/Data_FHN_U.npy').T[::2,::2,:] \n",
    "data_v_clean = np.load('/home/esaha/links/projects/def-wanghao/esaha/fhn_codes_data/Data_FHN_V.npy').T[::2,::2,:] \n",
    "N_x = data_u_clean.shape[0]-1\n",
    "N_y = N_x\n",
    "N_t = 50\n",
    "data_u_norm_clean = np.zeros((data_u_clean.shape[0],data_u_clean.shape[1],N_t))\n",
    "data_v_norm_clean = np.zeros((data_v_clean.shape[0],data_v_clean.shape[1],N_t))\n",
    "for i in range(N_t):\n",
    "    u_min_clean = np.min(data_u_clean[:,:,i])\n",
    "    u_max_clean = np.max(data_u_clean[:,:,i])\n",
    "    data_u_norm_clean[:,:,i] = (data_u_clean[:,:,i] - u_min_clean)/(u_max_clean - u_min_clean)\n",
    "\n",
    "    v_min_clean = np.min(data_v_clean[:,:,i])\n",
    "    v_max_clean = np.max(data_v_clean[:,:,i])\n",
    "    data_v_norm_clean[:,:,i] = (data_v_clean[:,:,i] - v_min_clean)/(v_max_clean - v_min_clean)\n",
    "\n",
    "\n",
    "data_u = data_u_clean #+ np.random.normal(0,noise,(100,100,50)) #for adding noise to the data\n",
    "data_v = data_v_clean #+ np.random.normal(0,noise,(100,100,50))\n",
    "print(data_u.shape)\n",
    "plt.imshow(data_u[:,:,5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba62a44",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538717d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bounds = np.linspace(0,1,N_t)\n",
    "x_bounds = np.linspace(0,1,N_x + 1)\n",
    "y_bounds = np.linspace(0,1,N_y + 1)\n",
    "data_u_norm = np.zeros((data_u.shape[0],data_u.shape[1],N_t))\n",
    "data_v_norm = np.zeros((data_v.shape[0],data_v.shape[1],N_t))\n",
    "u_min_vec = np.zeros(N_t)\n",
    "u_max_vec = np.zeros(N_t)\n",
    "v_min_vec = np.zeros(N_t)\n",
    "v_max_vec = np.zeros(N_t)\n",
    "\n",
    "for i in range(N_t):\n",
    "    u_min = np.min(data_u[:,:,i])\n",
    "    u_max = np.max(data_u[:,:,i])\n",
    "    data_u_norm[:,:,i] = (data_u[:,:,i] - u_min)/(u_max - u_min)\n",
    "\n",
    "    v_min = np.min(data_v[:,:,i])\n",
    "    v_max = np.max(data_v[:,:,i])\n",
    "    data_v_norm[:,:,i] = (data_v[:,:,i] - v_min)/(v_max - v_min)\n",
    "\n",
    "u_min_vec[i] = u_min\n",
    "u_max_vec[i] = u_max\n",
    "v_min_vec[i] = v_min\n",
    "v_max_vec[i] = v_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37331df5",
   "metadata": {},
   "source": [
    "### Create the training and validation dataset\n",
    "We first select $N_s$ random spatial points (note that $N_s<<nx$) and $steps$ random temporal points ($steps\\leq nt$) for building the training+validation set. Further the training-validation split is done based on a 80-20 split. \n",
    "The test set consists of all the points in the entire spatio-temporal domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede86d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_u = data_u_norm\n",
    "data_v = data_v_norm\n",
    "plt.plot(data_v_norm.reshape(-1))\n",
    "\n",
    "u_data = data_u[:,:,:].reshape((N_x+1)*(N_y+1),N_t)\n",
    "v_data = data_v[:,:,:].reshape((N_x+1)*(N_y+1),N_t)\n",
    "u0_data = data_u[:,:,0].reshape((N_x+1)*(N_y+1),1)\n",
    "v0_data = data_v[:,:,0].reshape((N_x+1)*(N_y+1),1)\n",
    "\n",
    "t_data = t_bounds[:N_t]\n",
    "t_data = np.tile(t_data,((N_x+1)*(N_y+1),1))\n",
    "\n",
    "\n",
    "x_data = x_bounds.reshape(-1,1)\n",
    "x_data = np.tile(x_data, (1, N_x+1))\n",
    "x_data = np.reshape(x_data, (-1, 1))\n",
    "x_data = np.tile(x_data, (1, N_t))\n",
    "\n",
    "y_data = y_bounds.reshape((1,-1)) #Note this reshape is (1,-1) and NOT (-1,1)\n",
    "y_data = np.tile(y_data, ((N_y+1), 1))\n",
    "y_data = np.reshape(y_data, (-1, 1))\n",
    "y_data = np.tile(y_data, (1, N_t))\n",
    "\n",
    "N_s = 1000\n",
    "steps = 40\n",
    "print('N_s and N_t are:',N_s,N_t)\n",
    "idx_s = np.random.choice(x_data.shape[0], N_s, replace = False)\n",
    "idx_t = np.random.choice(N_t,steps, replace = False)\n",
    "print(idx_t,'\\n\\n',idx_s)\n",
    "u_max = np.tile(u_max_vec,(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "u_min = np.tile(u_min_vec,(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "v_max = np.tile(v_max_vec,(((N_x+1)*(N_y+1)),1,1,1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "v_min = np.tile(v_min_vec,(((N_x+1)*(N_y+1)),1,1,1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "\n",
    "\n",
    "t_meas = t_data[idx_s, :]\n",
    "t_meas = t_meas[:, idx_t].reshape((-1,1))\n",
    "x_meas = x_data[idx_s, :]\n",
    "x_meas = x_meas[:, idx_t].reshape((-1,1))\n",
    "y_meas = y_data[idx_s, :]\n",
    "y_meas = y_meas[:, idx_t].reshape((-1,1))\n",
    "u_meas = u_data[idx_s, :]\n",
    "u_meas = u_meas[:, idx_t].reshape((-1,1))\n",
    "v_meas = v_data[idx_s, :]\n",
    "v_meas = v_meas[:, idx_t].reshape((-1,1))\n",
    "\n",
    "u_max_meas = u_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "u_min_meas = u_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "v_max_meas = v_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "v_min_meas = v_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "\n",
    "\n",
    "X_meas = np.hstack((x_meas, y_meas, t_meas))\n",
    "\n",
    "Split_TrainVal = 0.8\n",
    "N_train = int(N_s*steps*Split_TrainVal)\n",
    "idx_train = np.random.choice(X_meas.shape[0], N_train, replace=False)\n",
    "X_train = X_meas[idx_train,:]\n",
    "u_train = u_meas[idx_train,:]\n",
    "v_train = v_meas[idx_train,:]\n",
    "u_max_train = u_max_meas[idx_train,:]\n",
    "u_min_train = u_min_meas[idx_train,:]\n",
    "v_max_train = v_max_meas[idx_train,:]\n",
    "v_min_train = v_min_meas[idx_train,:]\n",
    "\n",
    "# Validation Measurements, which are the rest of measurements\n",
    "idx_val = np.setdiff1d(np.arange(X_meas.shape[0]), idx_train, assume_unique=True)\n",
    "X_val = X_meas[idx_val,:]\n",
    "u_val = u_meas[idx_val,:]\n",
    "v_val = v_meas[idx_val,:]\n",
    "u_max_val = u_max_meas[idx_val,:]\n",
    "u_min_val = u_min_meas[idx_val,:]\n",
    "v_max_val = v_max_meas[idx_val,:]\n",
    "v_min_val = v_min_meas[idx_val,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c285ffe",
   "metadata": {},
   "source": [
    "### Define relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cccea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weights_and_nnz(net):\n",
    "    total_weights = 0\n",
    "    total_nnz = 0\n",
    "\n",
    "    layers = [\n",
    "        (net.hidden_layer1, net.g1),\n",
    "        (net.hidden_layer2, net.g2),\n",
    "        (net.hidden_layer3, net.g3),\n",
    "        (net.hidden_layer4, net.g4),\n",
    "        (net.hidden_layer5, net.g5),\n",
    "        (net.hidden_layer6, net.g6),\n",
    "    ]\n",
    "\n",
    "    for layer, gate in layers:\n",
    "        w = layer.weight\n",
    "        b = layer.bias\n",
    "\n",
    "        in_features = w.shape[1]\n",
    "        out_features = w.shape[0]\n",
    "\n",
    "        # Count total weights in this layer\n",
    "        total_layer = w.numel() + b.numel()\n",
    "        total_weights += total_layer\n",
    "\n",
    "        # Expected active neurons from gate (soft L0)\n",
    "        s = torch.sigmoid(gate.qz_loga).detach()\n",
    "        active = (s > 0.5).float()  # hard threshold\n",
    "        n_active = int(active.sum().item())\n",
    "\n",
    "        # Each active neuron has all its incoming weights + 1 bias\n",
    "        nnz_layer = n_active * (in_features + 1)\n",
    "        total_nnz += nnz_layer\n",
    "\n",
    "    # Output layer (fully dense)\n",
    "    w = net.output_layer.weight\n",
    "    b = net.output_layer.bias\n",
    "    total_weights += w.numel() + b.numel()\n",
    "    total_nnz += w.numel() + b.numel()\n",
    "\n",
    "    return total_weights, total_nnz\n",
    "\n",
    "\n",
    "def m(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "def compute_residuals(u, v, x, y, t, umin, umax, vmin, vmax, L):\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), x , create_graph=True,allow_unused=True)[0]\n",
    "    u_y = torch.autograd.grad(u.sum(), y, create_graph=True,allow_unused=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y.sum(), y , create_graph=True,allow_unused=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True,allow_unused=True)[0]\n",
    "    v_t = torch.autograd.grad(v.sum(), t, create_graph=True,allow_unused=True)[0]\n",
    "    v_x = torch.autograd.grad(v.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "    v_xx = torch.autograd.grad(v_x.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "    v_y = torch.autograd.grad(v.sum(), y, create_graph=True,allow_unused=True)[0]\n",
    "    v_yy = torch.autograd.grad(v_y.sum(), y, create_graph=True,allow_unused=True)[0]\n",
    "    c_v = vmax-vmin\n",
    "    c_u = umax - umin\n",
    "\n",
    "    pde_u = (u_xx + u_yy + u - u**3 - v + 0.01 - u_t).reshape(-1,1)\n",
    "    pde_v = (100*c_v*(1/L**2)*v_xx + 100*c_v*(1/L**2)*v_yy + 0.25*c_u*u + 0.25*umin - 0.25*c_v*v - 0.25*vmin -c_v*(1/50)*v_t).reshape(-1,1)\n",
    "    return pde_u,pde_v\n",
    "\n",
    "class L0Gate(nn.Module):\n",
    "    def __init__(self, shape, droprate_init=0.5, temperature=2./3.):\n",
    "        super().__init__()\n",
    "        self.qz_loga = nn.Parameter(torch.Tensor(shape))\n",
    "        self.temperature = temperature\n",
    "        # init log-alpha\n",
    "        self.qz_loga.data.normal_(mean=np.log(droprate_init) - np.log(1 - droprate_init), std=1e-2)\n",
    "\n",
    "    def _hard_concrete_sample(self):\n",
    "        u = torch.rand_like(self.qz_loga)\n",
    "        s = torch.sigmoid((torch.log(u) - torch.log(1 - u) + self.qz_loga) / self.temperature)\n",
    "        z = s* (1.1 - 0.1) + 0.1  # Stretch to (0.1, 1.1)\n",
    "        return torch.clamp(z, 0, 1)\n",
    "\n",
    "    def forward(self):\n",
    "        return self._hard_concrete_sample()\n",
    "\n",
    "    def l0_loss(self):\n",
    "        # Expected gate value → expected L0 norm\n",
    "        s = torch.sigmoid(self.qz_loga)\n",
    "        return torch.sum(s)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden_layer1 = nn.Linear(3, H)\n",
    "        self.hidden_layer2 = nn.Linear(H, H)\n",
    "        self.hidden_layer3 = nn.Linear(H, H)\n",
    "        self.hidden_layer4 = nn.Linear(H, H)\n",
    "        self.hidden_layer5 = nn.Linear(H, H)\n",
    "        self.hidden_layer6 = nn.Linear(H, H)\n",
    "\n",
    "        # Add gates (one per neuron)\n",
    "        self.g1 = L0Gate((H,))\n",
    "        self.g2 = L0Gate((H,))\n",
    "        self.g3 = L0Gate((H,))\n",
    "        self.g4 = L0Gate((H,))\n",
    "        self.g5 = L0Gate((H,))\n",
    "        self.g6 = L0Gate((H,))\n",
    "\n",
    "        self.output_layer = nn.Linear(H, 2)\n",
    "\n",
    "    def forward(self, x, y, t, umin, umax, vmin, vmax):\n",
    "        inputs = torch.cat([x,y,t],axis=1)\n",
    "\n",
    "        z1 = self.g1()\n",
    "        z2 = self.g2()\n",
    "        z3 = self.g3()\n",
    "        z4 = self.g4()\n",
    "        z5 = self.g5()\n",
    "        z6 = self.g6()\n",
    "\n",
    "        h1 = m(self.hidden_layer1(inputs)) * z1\n",
    "        h2 = m(self.hidden_layer2(h1)) * z2\n",
    "        h3 = m(self.hidden_layer3(h2)) * z3\n",
    "        h4 = m(self.hidden_layer4(h3)) * z4\n",
    "        h5 = m(self.hidden_layer5(h4)) * z5\n",
    "        h6 = m(self.hidden_layer6(h5)) * z6\n",
    "\n",
    "        output = self.output_layer(h6)\n",
    "        u = output[:,0].reshape(-1,1)\n",
    "        v = output[:,1].reshape(-1,1)\n",
    "        pdeu=0\n",
    "        pdev=0\n",
    "\n",
    "        return u, v, pdeu, pdev\n",
    "\n",
    "\n",
    "\n",
    "def cart_inputs(x,y,t):\n",
    "    a = np.array([[x0, y0,t0] for x0 in x for y0 in y for t0 in t])\n",
    "    return a[:,0].reshape(-1,1), a[:,1].reshape(-1,1), a[:,2].reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c163d82f",
   "metadata": {},
   "source": [
    "### Create the input-output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533c392",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ic,y_ic,t_ic = cart_inputs(x_bounds,y_bounds,t_bounds[0]*np.ones((1)))\n",
    "\n",
    "u_ic = torch.tensor(u0_data).reshape(-1).reshape(-1,1).detach().numpy()\n",
    "v_ic = torch.tensor(v0_data).reshape(-1).reshape(-1,1).detach().numpy()\n",
    "\n",
    "x_collocation = X_train[:,0].reshape(-1,1) \n",
    "y_collocation = X_train[:,1].reshape(-1,1)\n",
    "t_collocation = X_train[:,2].reshape(-1,1)\n",
    "pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "pt_y_collocation = Variable(torch.from_numpy(y_collocation).float(), requires_grad=True).to(device)\n",
    "pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
    "\n",
    "pt_x_ic = Variable(torch.from_numpy(x_ic).float(), requires_grad=True).to(device)\n",
    "pt_y_ic = Variable(torch.from_numpy(y_ic).float(), requires_grad=True).to(device)\n",
    "pt_t_ic = Variable(torch.from_numpy(t_ic).float(), requires_grad=True).to(device)\n",
    "\n",
    "all_zeros = np.zeros((X_train.shape[0],1))\n",
    "pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
    "\n",
    "x_val = X_val[:,0].reshape(-1,1) \n",
    "y_val = X_val[:,1].reshape(-1,1) \n",
    "t_val = X_val[:,2].reshape(-1,1)\n",
    "\n",
    "pt_x_val = Variable(torch.from_numpy(x_val).float(), requires_grad=True).to(device)\n",
    "pt_y_val = Variable(torch.from_numpy(y_val).float(), requires_grad=True).to(device)\n",
    "pt_t_val = Variable(torch.from_numpy(t_val).float(), requires_grad=True).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7c34c2",
   "metadata": {},
   "source": [
    "### Full training-valdation and evaluation loop for different hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d8ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparameters ===\n",
    "mse_cost_function1 = torch.nn.MSELoss() # Mean squared error\n",
    "learning_rates = [5e-03]\n",
    "# hidden_dims = [200]\n",
    "hidden_dim = 100\n",
    "lam_0 = [1e-2,1e-4]\n",
    "num_repeats = 3\n",
    "validate_every = 5000\n",
    "max_epochs = 100000\n",
    "patience = 5\n",
    "\n",
    "best_global_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_hparams = {}\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # for hidden_dim in hidden_dims:\n",
    "    for lambda0 in lam_0:\n",
    "        run_errors = []\n",
    "\n",
    "        print(f\"\\n=== Training with lr={lr}, L0={lambda0} ===\")\n",
    "\n",
    "        for run in range(num_repeats):\n",
    "            print(f\"Run {run + 1}/{num_repeats}\")\n",
    "            torch.manual_seed(run)\n",
    "\n",
    "            t_start_train = time.time()\n",
    "            net = Net(hidden_dim).to(device)\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            for epoch in range(0, max_epochs):\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                uout, vout, f_out1, g_out1 = net(\n",
    "                    pt_x_collocation, pt_y_collocation, pt_t_collocation,\n",
    "                    torch.tensor(u_min_train).float().to(device),\n",
    "                    torch.tensor(u_max_train).float().to(device),\n",
    "                    torch.tensor(v_min_train).float().to(device),\n",
    "                    torch.tensor(v_max_train).float().to(device)\n",
    "                )\n",
    "                f_out, g_out = compute_residuals(\n",
    "                    uout, vout,\n",
    "                    pt_x_collocation, pt_y_collocation, pt_t_collocation,\n",
    "                    torch.tensor(u_min_train).float().to(device),\n",
    "                    torch.tensor(u_max_train).float().to(device),\n",
    "                    torch.tensor(v_min_train).float().to(device),\n",
    "                    torch.tensor(v_max_train).float().to(device),\n",
    "                    L\n",
    "                )\n",
    "\n",
    "                net_uic, net_vic, _, _ = net(\n",
    "                    pt_x_ic, pt_y_ic, pt_t_ic,\n",
    "                    torch.tensor(u_min[:, 0]).float().to(device),\n",
    "                    torch.tensor(u_max[:, 0]).float().to(device),\n",
    "                    torch.tensor(v_min[:, 0]).float().to(device),\n",
    "                    torch.tensor(v_max[:, 0]).float().to(device)\n",
    "                )\n",
    "\n",
    "                u_out = uout.reshape(-1, 1)\n",
    "                v_out = vout.reshape(-1, 1)\n",
    "\n",
    "                mse_v = mse_cost_function1(g_out, pt_all_zeros)\n",
    "                mse_vic = mse_cost_function1(net_vic.reshape(-1, 1), torch.from_numpy(v_ic.reshape(-1, 1)).float().to(device))\n",
    "                mse_udata = mse_cost_function1(u_out, torch.tensor(u_train).float().to(device))\n",
    "                L0_term = (net.g1.l0_loss() + net.g2.l0_loss() + net.g3.l0_loss() + net.g4.l0_loss() \n",
    "                            + net.g5.l0_loss() + net.g6.l0_loss())\n",
    "\n",
    "               \n",
    "                loss = mse_udata + mse_v + mse_vic + lambda0 * L0_term\n",
    "                # loss = mse_udata # + mse_vic\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % validate_every == 0:\n",
    "                    net.eval()\n",
    "                    # with torch.no_grad():\n",
    "                    u_outval, v_outval, _, _ = net(\n",
    "                        pt_x_val, pt_y_val, pt_t_val,\n",
    "                        torch.tensor(u_min_val).float().to(device),\n",
    "                        torch.tensor(u_max_val).float().to(device),\n",
    "                        torch.tensor(v_min_val).float().to(device),\n",
    "                        torch.tensor(v_max_val).float().to(device)\n",
    "                    )\n",
    "\n",
    "                    rel_u = torch.norm(torch.tensor(u_val).reshape(-1).to(device) - u_outval.reshape(-1)) / \\\n",
    "                            torch.norm(torch.tensor(u_val).reshape(-1).to(device))\n",
    "\n",
    "                    rel_v = torch.norm(torch.tensor(v_val).reshape(-1).to(device) - v_outval.reshape(-1)) / \\\n",
    "                            torch.norm(torch.tensor(v_val).reshape(-1).to(device))\n",
    "\n",
    "                    val_loss = 0.5 * (rel_u + rel_v)\n",
    "                    # val_loss = rel_u\n",
    "                    print(val_loss)\n",
    "\n",
    "                    if val_loss.item() < best_val_loss - 1e-6:\n",
    "                        best_val_loss = val_loss.item()\n",
    "                        best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                        epochs_no_improve = 0\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "\n",
    "                    if epochs_no_improve >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "            t_end_train = time.time()\n",
    "            # Load best weights\n",
    "            net.load_state_dict(best_model_wts)\n",
    "\n",
    "            # Evaluate final validation score\n",
    "            s=1\n",
    "            xx= x_bounds[::s] \n",
    "            yy= y_bounds[::s] \n",
    "            tt= t_data[0,:]\n",
    "            x1,y1,t1 = cart_inputs(xx,yy,tt)\n",
    "            pt_x = Variable(torch.from_numpy(x1).float(), requires_grad=True).to(device)\n",
    "            pt_y = Variable(torch.from_numpy(y1).float(), requires_grad=True).to(device)\n",
    "            pt_t = Variable(torch.from_numpy(t1).float(), requires_grad=True).to(device)\n",
    "            t_start_eval = time.time()\n",
    "            pt_u,pt_v,_,_ = net(pt_x,pt_y,pt_t,torch.tensor(u_min[::s,::s]).reshape(-1,1).float().to(device),\n",
    "                                            torch.tensor(u_max[::s,::s]).reshape(-1,1).float().to(device),torch.tensor(v_min[::s,::s]).reshape(-1,1).float().to(device),\n",
    "                                            torch.tensor(v_max[::s,::s]).reshape(-1,1).float().to(device)) #.detach().numpy()\n",
    "            t_end_eval = time.time()\n",
    "            ms_u = pt_u.reshape(xx.shape[0],yy.shape[0],tt.shape[0])\n",
    "            ms_v = pt_v.reshape(xx.shape[0],yy.shape[0],tt.shape[0])\n",
    "            full_field_true = torch.hstack((torch.tensor(data_u_norm_clean),torch.tensor(data_v_norm_clean))).to(device)\n",
    "            full_field_net = torch.hstack((ms_u,ms_v)).detach()\n",
    "\n",
    "            error_uv = torch.zeros(N_t)\n",
    "            error_v = torch.zeros(N_t)\n",
    "            error_u = torch.zeros(N_t)\n",
    "            for i in range(N_t):\n",
    "                error_uv[i] = torch.norm(full_field_true[:,:,i] - full_field_net[:,:,i])/torch.norm(full_field_true[:,:,i])\n",
    "                error_v[i] = torch.norm(torch.tensor(data_v_norm_clean[::s,::s,i]).to(device) - ms_v[:,:,i])/torch.norm(torch.tensor(data_v_norm_clean[::s,::s,i]).to(device))\n",
    "                error_u[i] = torch.norm(torch.tensor(data_u_norm_clean[::s,::s,i]).to(device) - ms_u[:,:,i])/torch.norm(torch.tensor(data_u_norm_clean[::s,::s,i]).to(device))\n",
    "\n",
    "            print('\\nError uv',torch.mean(error_uv),' Error u',torch.mean(error_u),'Error v:', torch.mean(error_v),'\\n')\n",
    "\n",
    "            final_error = torch.mean(error_uv)\n",
    "            run_errors.append(final_error.item())\n",
    "\n",
    "            print(f\"Run {run+1}/{num_repeats} Final Val Error: {final_error.item():.6f}\")\n",
    "            total_w, nnz_w = count_weights_and_nnz(net)\n",
    "            perc = 100 * nnz_w / total_w\n",
    "\n",
    "            print(f\"\\nTotal weights: {total_w}\")\n",
    "            print(f\"\\nNon-zero (active) weights: {nnz_w}\")\n",
    "            print(f\"\\nPercentage active: {perc:.2f}%\")\n",
    "            print(f\"\\nTotal training time is\",t_end_train-t_start_train)\n",
    "            print(f\"\\nTotal Evaluation time is\", t_end_eval - t_start_eval)\n",
    "            print(\"\\n---------------------------------------------------------\\n\")\n",
    "\n",
    "        # Compute mean ± 95% CI\n",
    "        mean_error = np.mean(run_errors)\n",
    "        sem = stats.sem(run_errors)\n",
    "        ci95 = sem * stats.t.ppf((1 + 0.95) / 2., num_repeats - 1)\n",
    "\n",
    "        print(f\"At lr={lr}, hidden_dim={hidden_dim} mean full-field L2 Error: {mean_error:.6f} ± {ci95:.6f}\")\n",
    "        results.append((lr, hidden_dim, mean_error, ci95))\n",
    "\n",
    "        if mean_error < best_global_val_loss:\n",
    "            best_global_val_loss = mean_error\n",
    "            best_model_state = copy.deepcopy(net.state_dict())\n",
    "            best_hparams = {'lr': lr, 'hidden_dim': hidden_dim}\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_hparams}, Validation Error: {best_global_val_loss:.6f}\")\n",
    "torch.save(net,f'/home/esaha/links/scratch/L0-trained-models-outputs/fhn-{N_s}Ns-lr-{lr}-L0-{lambda0}-4Layers-{hidden_dim}dim-Best')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
