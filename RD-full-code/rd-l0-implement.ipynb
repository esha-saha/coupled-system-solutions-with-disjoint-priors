{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266aa19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "# from pyDOE2 import lhs\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70243c2",
   "metadata": {},
   "source": [
    "### Load data and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.00 #standard deviation of Gaussian noise added to data (if applicable)\n",
    "L = 20.0\n",
    "xs = 2\n",
    "xt = 2\n",
    "data_u_clean = np.load('/RD-full-code/Reaction_Diff_spiral_U.npy')[::xs,::xs,::xt] #see README.md for access to data\n",
    "data_v_clean = np.load('/RD-full-code/Reaction_Diff_spiral_V.npy')[::xs,::xs,::xt] #see README.md for access to data \n",
    "data_u = data_u_clean + np.random.normal(0,noise,(128,128,101))\n",
    "data_v = data_v_clean + np.random.normal(0,noise,(128,128,101))\n",
    "\n",
    "N_x = data_u.shape[0]-1\n",
    "N_y = N_x\n",
    "N_t = data_u.shape[2]\n",
    "\n",
    "\n",
    "t_bounds_true = np.load('/RD-full-code/Reaction_Diff_spiral_t.npy').T.reshape(-1)[::xt]\n",
    "tmin = np.min(t_bounds_true)\n",
    "tmax = np.max(t_bounds_true)\n",
    "t_bounds = t_bounds_true #(t_bounds_true - tmin)/(tmax-tmin)\n",
    "print(t_bounds.shape)\n",
    "\n",
    "x_bounds_true = np.load('/RD-full-code/Reaction_Diff_spiral_X.npy').T.reshape(-1)[::xs]\n",
    "xmin = np.min(x_bounds_true)\n",
    "xmax = np.max(x_bounds_true)\n",
    "x_bounds = x_bounds_true #(x_bounds_true - xmin)/(xmax-xmin)\n",
    "\n",
    "y_bounds_true = np.load('/RD-full-code/Reaction_Diff_spiral_Y.npy').T.reshape(-1)[::xs]\n",
    "ymin = np.min(y_bounds_true)\n",
    "ymax = np.max(y_bounds_true)\n",
    "y_bounds = y_bounds_true #(y_bounds_true - ymin)/(ymax-ymin)\n",
    "\n",
    "data_u_norm = np.zeros((data_u.shape[0],data_u.shape[1],N_t))\n",
    "data_v_norm = np.zeros((data_v.shape[0],data_v.shape[1],N_t))\n",
    "u_min_vec = np.zeros(N_t)\n",
    "u_max_vec = np.zeros(N_t)\n",
    "v_min_vec = np.zeros(N_t)\n",
    "v_max_vec = np.zeros(N_t)\n",
    "\n",
    "for i in range(N_t):\n",
    "    u_min = np.min(data_u[:,:,i])\n",
    "    u_max = np.max(data_u[:,:,i])\n",
    "    data_u_norm[:,:,i] = (data_u[:,:,i] - u_min)/(u_max - u_min)\n",
    "\n",
    "    v_min = np.min(data_v[:,:,i])\n",
    "    v_max = np.max(data_v[:,:,i])\n",
    "    data_v_norm[:,:,i] = (data_v[:,:,i] - v_min)/(v_max - v_min)\n",
    "\n",
    "u_min_vec[i] = u_min\n",
    "u_max_vec[i] = u_max\n",
    "v_min_vec[i] = v_min\n",
    "v_max_vec[i] = v_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52c065d",
   "metadata": {},
   "source": [
    "### Create training and validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_data = data_u[:,:,:].reshape((N_x+1)*(N_y+1),N_t)\n",
    "v_data = data_v[:,:,:].reshape((N_x+1)*(N_y+1),N_t)\n",
    "u0_data = data_u[:,:,0].reshape((N_x+1)*(N_y+1),1)\n",
    "v0_data = data_v[:,:,0].reshape((N_x+1)*(N_y+1),1)\n",
    "\n",
    "t_data = t_bounds #[:N_t]\n",
    "t_data = np.tile(t_data,((N_x+1)*(N_y+1),1))\n",
    "\n",
    "\n",
    "x_data = x_bounds.reshape(-1,1)\n",
    "x_data = np.tile(x_data, (1, N_x+1))\n",
    "x_data = np.reshape(x_data, (-1, 1))\n",
    "x_data = np.tile(x_data, (1, N_t))\n",
    "\n",
    "y_data = y_bounds.reshape((1,-1)) #Note this reshape is (1,-1) and NOT (-1,1)\n",
    "y_data = np.tile(y_data, ((N_y+1), 1))\n",
    "y_data = np.reshape(y_data, (-1, 1))\n",
    "y_data = np.tile(y_data, (1, N_t))\n",
    "\n",
    "N_s = 500\n",
    "steps = 100 #N_t-1\n",
    "idx_s = np.random.choice(x_data.shape[0], N_s, replace = False)\n",
    "idx_t = np.random.choice(N_t,steps, replace = False)\n",
    "u_max = np.tile(u_max_vec,(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "u_min = np.tile(u_min_vec,(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "v_max = np.tile(v_max_vec,(((N_x+1)*(N_y+1)),1,1,1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "v_min = np.tile(v_min_vec,(((N_x+1)*(N_y+1)),1,1,1)).squeeze().reshape((N_x+1)*(N_y+1),N_t)\n",
    "\n",
    "t_meas = t_data[idx_s, :]\n",
    "t_meas = t_meas[:, idx_t].reshape((-1,1))\n",
    "x_meas = x_data[idx_s, :]\n",
    "x_meas = x_meas[:, idx_t].reshape((-1,1))\n",
    "y_meas = y_data[idx_s, :]\n",
    "y_meas = y_meas[:, idx_t].reshape((-1,1))\n",
    "u_meas = u_data[idx_s, :]\n",
    "u_meas = u_meas[:, idx_t].reshape((-1,1))\n",
    "v_meas = v_data[idx_s, :]\n",
    "v_meas = v_meas[:, idx_t].reshape((-1,1))\n",
    "\n",
    "u_max_meas = u_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "u_min_meas = u_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "v_max_meas = v_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "v_min_meas = v_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "\n",
    "X_meas = np.hstack((x_meas, y_meas, t_meas))\n",
    "\n",
    "Split_TrainVal = 0.8\n",
    "N_train = int(N_s*steps*Split_TrainVal)\n",
    "idx_train = np.random.choice(X_meas.shape[0], N_train, replace=False)\n",
    "X_train = X_meas[idx_train,:]\n",
    "u_train = u_meas[idx_train,:]\n",
    "v_train = v_meas[idx_train,:]\n",
    "u_max_train = u_max_meas[idx_train,:]\n",
    "u_min_train = u_min_meas[idx_train,:]\n",
    "v_max_train = v_max_meas[idx_train,:]\n",
    "v_min_train = v_min_meas[idx_train,:]\n",
    "\n",
    "# Validation Measurements, which are the rest of measurements\n",
    "idx_val = np.setdiff1d(np.arange(X_meas.shape[0]), idx_train, assume_unique=True)\n",
    "X_val = X_meas[idx_val,:]\n",
    "u_val = u_meas[idx_val,:]\n",
    "v_val = v_meas[idx_val,:]\n",
    "u_max_val = u_max_meas[idx_val,:]\n",
    "u_min_val = u_min_meas[idx_val,:]\n",
    "v_max_val = v_max_meas[idx_val,:]\n",
    "v_min_val = v_min_meas[idx_val,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f36cf",
   "metadata": {},
   "source": [
    "### Define relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6066c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_weights_and_nnz(net):\n",
    "    total_weights = 0\n",
    "    total_nnz = 0\n",
    "\n",
    "    layers = [\n",
    "        (net.hidden_layer1, net.g1),\n",
    "        (net.hidden_layer2, net.g2),\n",
    "        (net.hidden_layer3, net.g3),\n",
    "        (net.hidden_layer4, net.g4),\n",
    "        (net.hidden_layer5, net.g5),\n",
    "        (net.hidden_layer6, net.g6),\n",
    "    ]\n",
    "\n",
    "    for layer, gate in layers:\n",
    "        w = layer.weight\n",
    "        b = layer.bias\n",
    "\n",
    "        in_features = w.shape[1]\n",
    "        out_features = w.shape[0]\n",
    "\n",
    "        # Count total weights in this layer\n",
    "        total_layer = w.numel() + b.numel()\n",
    "        total_weights += total_layer\n",
    "\n",
    "        # Expected active neurons from gate (soft L0)\n",
    "        s = torch.sigmoid(gate.qz_loga).detach()\n",
    "        active = (s > 0.5).float()  # hard threshold\n",
    "        n_active = int(active.sum().item())\n",
    "\n",
    "        # Each active neuron has all its incoming weights + 1 bias\n",
    "        nnz_layer = n_active * (in_features + 1)\n",
    "        total_nnz += nnz_layer\n",
    "\n",
    "    # Output layer (fully dense)\n",
    "    w = net.output_layer.weight\n",
    "    b = net.output_layer.bias\n",
    "    total_weights += w.numel() + b.numel()\n",
    "    total_nnz += w.numel() + b.numel()\n",
    "\n",
    "    return total_weights, total_nnz\n",
    "\n",
    "\n",
    "def m(x):\n",
    "    return torch.sin(x)\n",
    "\n",
    "def compute_residuals(u, v, x, y, t, umin, umax, vmin, vmax, L):\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), x , create_graph=True,allow_unused=True)[0]\n",
    "    u_y = torch.autograd.grad(u.sum(), y, create_graph=True,allow_unused=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y.sum(), y , create_graph=True,allow_unused=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True,allow_unused=True)[0]\n",
    "    c_v = vmax-vmin\n",
    "    c_u = umax - umin\n",
    "\n",
    "    pde_u = (0.1*u_xx + 0.1*u_yy - u*v**2 - u**3 + v**3 +u**2*v + u - u_t).reshape(-1,1)\n",
    "    pde_v = 0 #replace with equation of using this as equation variable\n",
    "    return pde_u,pde_v\n",
    "\n",
    "class L0Gate(nn.Module):\n",
    "    def __init__(self, shape, droprate_init=0.5, temperature=2./3.):\n",
    "        super().__init__()\n",
    "        self.qz_loga = nn.Parameter(torch.Tensor(shape))\n",
    "        self.temperature = temperature\n",
    "        # init log-alpha\n",
    "        self.qz_loga.data.normal_(mean=np.log(droprate_init) - np.log(1 - droprate_init), std=1e-2)\n",
    "\n",
    "    def _hard_concrete_sample(self):\n",
    "        u = torch.rand_like(self.qz_loga)\n",
    "        s = torch.sigmoid((torch.log(u) - torch.log(1 - u) + self.qz_loga) / self.temperature)\n",
    "        # z = s* (1.1 - 0.7) + 0.1  # Stretch to (0.1, 1.1)\n",
    "        z = s* (1.1 - 0.1) + 0.1  # Stretch to (0.1, 1.1)\n",
    "        return torch.clamp(z, 0, 1)\n",
    "\n",
    "    def forward(self):\n",
    "        return self._hard_concrete_sample()\n",
    "\n",
    "    def l0_loss(self):\n",
    "        # Expected gate value → expected L0 norm\n",
    "        s = torch.sigmoid(self.qz_loga)\n",
    "        return torch.sum(s)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden_layer1 = nn.Linear(3, H)\n",
    "        self.hidden_layer2 = nn.Linear(H, H)\n",
    "        self.hidden_layer3 = nn.Linear(H, H)\n",
    "        self.hidden_layer4 = nn.Linear(H, H)\n",
    "        self.hidden_layer5 = nn.Linear(H, H)\n",
    "        self.hidden_layer6 = nn.Linear(H, H)\n",
    "\n",
    "        # Add gates (one per neuron)\n",
    "        self.g1 = L0Gate((H,))\n",
    "        self.g2 = L0Gate((H,))\n",
    "        self.g3 = L0Gate((H,))\n",
    "        self.g4 = L0Gate((H,))\n",
    "        self.g5 = L0Gate((H,))\n",
    "        self.g6 = L0Gate((H,))\n",
    "\n",
    "        self.output_layer = nn.Linear(H, 2)\n",
    "\n",
    "    def forward(self, x, y, t, umin, umax, vmin, vmax):\n",
    "        inputs = torch.cat([x,y,t],axis=1)\n",
    "\n",
    "        z1 = self.g1()\n",
    "        z2 = self.g2()\n",
    "        z3 = self.g3()\n",
    "        z4 = self.g4()\n",
    "        z5 = self.g5()\n",
    "        z6 = self.g6()\n",
    "\n",
    "        h1 = m(self.hidden_layer1(inputs)) * z1\n",
    "        h2 = m(self.hidden_layer2(h1)) * z2\n",
    "        h3 = m(self.hidden_layer3(h2)) * z3\n",
    "        h4 = m(self.hidden_layer4(h3)) * z4\n",
    "        h5 = m(self.hidden_layer5(h4)) * z5\n",
    "        h6 = m(self.hidden_layer6(h5)) * z6\n",
    "\n",
    "        output = self.output_layer(h6)\n",
    "        u = output[:,0].reshape(-1,1)\n",
    "        v = output[:,1].reshape(-1,1)\n",
    "        pdeu=0\n",
    "        pdev=0\n",
    "\n",
    "        return u, v, pdeu, pdev\n",
    "\n",
    "\n",
    "def cart_inputs(x,y,t):\n",
    "    a = np.array([[x0, y0,t0] for x0 in x for y0 in y for t0 in t])\n",
    "    return a[:,0].reshape(-1,1), a[:,1].reshape(-1,1), a[:,2].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4950ce",
   "metadata": {},
   "source": [
    "### Create input-output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ddbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ic,y_ic,t_ic = cart_inputs(x_bounds,y_bounds,t_bounds[0]*np.ones((1)))\n",
    "\n",
    "u_ic = torch.tensor(u0_data).reshape(-1).reshape(-1,1).detach().numpy()\n",
    "v_ic = torch.tensor(v0_data).reshape(-1).reshape(-1,1).detach().numpy()\n",
    "\n",
    "x_collocation = X_train[:,0].reshape(-1,1) #np.random.uniform(low=x_min, high=x_max, size=(N_x+1,1))\n",
    "y_collocation = X_train[:,1].reshape(-1,1)\n",
    "t_collocation = X_train[:,2].reshape(-1,1)\n",
    "pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "pt_y_collocation = Variable(torch.from_numpy(y_collocation).float(), requires_grad=True).to(device)\n",
    "pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
    "\n",
    "pt_x_ic = Variable(torch.from_numpy(x_ic).float(), requires_grad=True).to(device)\n",
    "pt_y_ic = Variable(torch.from_numpy(y_ic).float(), requires_grad=True).to(device)\n",
    "pt_t_ic = Variable(torch.from_numpy(t_ic).float(), requires_grad=True).to(device)\n",
    "\n",
    "all_zeros = np.zeros((X_train.shape[0],1))\n",
    "pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
    "\n",
    "x_val = X_val[:,0].reshape(-1,1) #np.random.uniform(low=x_min, high=x_max, size=(N_x+1,1))\n",
    "y_val = X_val[:,1].reshape(-1,1) #np.random.uniform(low=y_min, high=y_max, size=(N_y+1,1))\n",
    "t_val = X_val[:,2].reshape(-1,1)\n",
    "\n",
    "pt_x_val = Variable(torch.from_numpy(x_val).float(), requires_grad=True).to(device)\n",
    "pt_y_val = Variable(torch.from_numpy(y_val).float(), requires_grad=True).to(device)\n",
    "pt_t_val = Variable(torch.from_numpy(t_val).float(), requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896a06d",
   "metadata": {},
   "source": [
    "### Full training, validation and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a46886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparameters ===\n",
    "learning_rates = [5e-3]\n",
    "lr = 5e-3\n",
    "# hidden_dims = [50]\n",
    "hidden_dims = [100]\n",
    "lam_0 = [1e-4,1e-6]\n",
    "num_repeats = 3\n",
    "validate_every = 5000\n",
    "max_epochs = 100000\n",
    "patience = 5\n",
    "\n",
    "best_global_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_hparams = {}\n",
    "\n",
    "results = []\n",
    "mse_cost_function1 = torch.nn.MSELoss() # Mean squared error\n",
    "\n",
    "# for lr in learning_rates:\n",
    "for hidden_dim in hidden_dims:\n",
    "    for lambda0 in lam_0:\n",
    "        run_errors = []\n",
    "\n",
    "        print(f\"\\n=== Training with lr={lr}, L0={lambda0} ===, noise={noise}\")\n",
    "\n",
    "        for run in range(num_repeats):\n",
    "            print(f\"Run {run + 1}/{num_repeats}\")\n",
    "            torch.manual_seed(run)\n",
    "\n",
    "            net = Net(hidden_dim).to(device)\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay = 0.0001)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "            t_start_train = time.time()\n",
    "\n",
    "            for epoch in range(0, max_epochs):\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "\n",
    "                uout, vout, f_out1, g_out1 = net(\n",
    "                    pt_x_collocation, pt_y_collocation, pt_t_collocation,\n",
    "                    torch.tensor(u_min_train).float().to(device),\n",
    "                    torch.tensor(u_max_train).float().to(device),\n",
    "                    torch.tensor(v_min_train).float().to(device),\n",
    "                    torch.tensor(v_max_train).float().to(device)\n",
    "                )\n",
    "                f_out, g_out = compute_residuals(\n",
    "                    uout, vout,\n",
    "                    pt_x_collocation, pt_y_collocation, pt_t_collocation,\n",
    "                    torch.tensor(u_min_train).float().to(device),\n",
    "                    torch.tensor(u_max_train).float().to(device),\n",
    "                    torch.tensor(v_min_train).float().to(device),\n",
    "                    torch.tensor(v_max_train).float().to(device),\n",
    "                    L\n",
    "                )\n",
    "\n",
    "                net_uic, net_vic, _, _ = net(\n",
    "                    pt_x_ic, pt_y_ic, pt_t_ic,\n",
    "                    torch.tensor(u_min[:, 0]).float().to(device),\n",
    "                    torch.tensor(u_max[:, 0]).float().to(device),\n",
    "                    torch.tensor(v_min[:, 0]).float().to(device),\n",
    "                    torch.tensor(v_max[:, 0]).float().to(device)\n",
    "                )\n",
    "\n",
    "                u_out = uout.reshape(-1, 1)\n",
    "                v_out = vout.reshape(-1, 1)\n",
    "\n",
    "                mse_u = mse_cost_function1(f_out, pt_all_zeros)\n",
    "                mse_uic = mse_cost_function1(net_uic.reshape(-1, 1), torch.from_numpy(u_ic.reshape(-1, 1)).float().to(device))\n",
    "                mse_vdata = mse_cost_function1(v_out, torch.tensor(v_train).float().to(device))\n",
    "                # Compute L0 loss (sum of expected active neurons)\n",
    "                L0_term = (net.g1.l0_loss() + net.g2.l0_loss() + net.g3.l0_loss() +\n",
    "                    net.g4.l0_loss() + net.g5.l0_loss() + net.g6.l0_loss())\n",
    "\n",
    "                # Weight the regularization\n",
    "                # lambda_l0 = 1e-5  # tune this\n",
    "\n",
    "                loss = mse_vdata + mse_u + mse_uic + lambda0 * L0_term\n",
    "\n",
    "\n",
    "                # loss = mse_vdata + mse_u + mse_uic\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % validate_every == 0:\n",
    "                    net.eval()\n",
    "                    # with torch.no_grad():\n",
    "                    u_outval, v_outval, _, _ = net(\n",
    "                        pt_x_val, pt_y_val, pt_t_val,\n",
    "                        torch.tensor(u_min_val).float().to(device),\n",
    "                        torch.tensor(u_max_val).float().to(device),\n",
    "                        torch.tensor(v_min_val).float().to(device),\n",
    "                        torch.tensor(v_max_val).float().to(device)\n",
    "                    )\n",
    "                #   full_uv_val = torch.hstack((torch.tensor(u_val),torch.tensor(v_val)))\n",
    "                #   net_uv_val = torch.hstack((u_outval,v_outval)).detach()\n",
    "                #   rel_uv = torch.norm(torch.tensor(full_uv_val).reshape(-1).to(device) - net_uv_val.reshape(-1)) / \\\n",
    "                #           torch.norm(torch.tensor(full_uv_val).reshape(-1).to(device))\n",
    "\n",
    "                    rel_u = torch.norm(torch.tensor(u_val).reshape(-1).to(device) - u_outval.reshape(-1)) / \\\n",
    "                            torch.norm(torch.tensor(u_val).reshape(-1).to(device))\n",
    "\n",
    "                    rel_v = torch.norm(torch.tensor(v_val).reshape(-1).to(device) - v_outval.reshape(-1)) / \\\n",
    "                            torch.norm(torch.tensor(v_val).reshape(-1).to(device))\n",
    "\n",
    "                    val_loss = 0.5 * (rel_u + rel_v)\n",
    "                    # val_loss = (rel_v)\n",
    "                    print(rel_v.item())\n",
    "\n",
    "                    if val_loss.item() < best_val_loss - 1e-6:\n",
    "                        best_val_loss = val_loss.item()\n",
    "                        best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                        epochs_no_improve = 0\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "\n",
    "                    if epochs_no_improve >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "            t_end_train = time.time()\n",
    "            # # Load best weights\n",
    "            net.load_state_dict(best_model_wts)\n",
    "\n",
    "            # Evaluate final validation score\n",
    "            s=1\n",
    "            xx= x_bounds[::s] #np.linspace(x_min,x_max,41)\n",
    "            yy= y_bounds[::s] #np.linspace(x_min,x_max,41)\n",
    "            tt= t_data[0,:] #np.linspace(0,10,100)\n",
    "            x1,y1,t1 = cart_inputs(xx,yy,tt)\n",
    "            pt_x = Variable(torch.from_numpy(x1).float(), requires_grad=True).to(device)\n",
    "            pt_y = Variable(torch.from_numpy(y1).float(), requires_grad=True).to(device)\n",
    "            pt_t = Variable(torch.from_numpy(t1).float(), requires_grad=True).to(device)\n",
    "            t_start_eval = time.time()\n",
    "            pt_u,pt_v,_,_ = net(pt_x,pt_y,pt_t,torch.tensor(u_min[::s,::s]).reshape(-1,1).float().to(device),\n",
    "                                            torch.tensor(u_max[::s,::s]).reshape(-1,1).float().to(device),torch.tensor(v_min[::s,::s]).reshape(-1,1).float().to(device),\n",
    "                                            torch.tensor(v_max[::s,::s]).reshape(-1,1).float().to(device)) #.detach().numpy()\n",
    "            t_end_eval = time.time()\n",
    "            ms_u = pt_u.reshape(xx.shape[0],yy.shape[0],tt.shape[0])\n",
    "            ms_v = pt_v.reshape(xx.shape[0],yy.shape[0],tt.shape[0])\n",
    "            \n",
    "\n",
    "            full_field_true = torch.hstack((torch.tensor(data_u_clean),torch.tensor(data_v_clean))).to(device)\n",
    "            full_field_net = torch.hstack((ms_u,ms_v)).detach()\n",
    "\n",
    "\n",
    "            error_uv = torch.zeros(N_t)\n",
    "            error_v = torch.zeros(N_t)\n",
    "            error_u = torch.zeros(N_t)\n",
    "            for i in range(N_t):\n",
    "            error_uv[i] = torch.norm(full_field_true[:,:,i] - full_field_net[:,:,i])/torch.norm(full_field_true[:,:,i])\n",
    "            error_v[i] = torch.norm(torch.tensor(data_v_clean[::s,::s,i]).to(device) - ms_v[:,:,i])/torch.norm(torch.tensor(data_v_clean[::s,::s,i]).to(device))\n",
    "            error_u[i] = torch.norm(torch.tensor(data_u_clean[::s,::s,i]).to(device) - ms_u[:,:,i])/torch.norm(torch.tensor(data_u_clean[::s,::s,i]).to(device))\n",
    "\n",
    "        #   error_u = torch.zeros(N_t)\n",
    "        #   error_v = torch.zeros(N_t)\n",
    "        #   for i in range(N_t):\n",
    "        #     error_u[i] = torch.norm(torch.tensor(data_u[::s,::s,i]).to(device) - ms_u[:,:,i].detach())/torch.norm(torch.tensor(data_u[::s,::s,i]).to(device))\n",
    "        #     error_v[i] = torch.norm(torch.tensor(data_v[::s,::s,i]).to(device) - ms_v[:,:,i].detach())/torch.norm(torch.tensor(data_v[::s,::s,i]).to(device))\n",
    "\n",
    "            print('\\nError uv',torch.mean(error_uv),' Error u',torch.mean(error_u),'Error v:', torch.mean(error_v),'\\n')\n",
    "\n",
    "            final_error = torch.mean(error_uv) #+ torch.mean(error_v))\n",
    "            run_errors.append(final_error.item())\n",
    "\n",
    "            print(f\"Run {run+1}/{num_repeats} Final Val Error: {final_error.item():.6f}\")\n",
    "            # print(f\"At lr={lr}, hidden_dim={hidden_dim} mean full-field L2 Error: {mean_error:.6f} ± {ci95:.6f}\")\n",
    "            total_w, nnz_w = count_weights_and_nnz(net)\n",
    "            perc = 100 * nnz_w / total_w\n",
    "\n",
    "            print(f\"\\nTotal weights: {total_w}\")\n",
    "            print(f\"Non-zero (active) weights: {nnz_w}\")\n",
    "            print(f\"Percentage active: {perc:.2f}%\")\n",
    "            print(f\"Total training time is\",t_end_train-t_start_train)\n",
    "            print(f\"\\nTotal Evaluation time is\", t_end_eval - t_start_eval)\n",
    "            print(\"-----------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "        # Compute mean ± 95% CI\n",
    "        mean_error = np.mean(run_errors)\n",
    "        sem = stats.sem(run_errors)\n",
    "        ci95 = sem * stats.t.ppf((1 + 0.95) / 2., num_repeats - 1)\n",
    "        print(f\"At lr={lr}, L0={lambda0} mean full-field L2 Error: {mean_error:.6f} ± {ci95:.6f}\")\n",
    "\n",
    "\n",
    "        # torch.save(net,f'/home/esaha/links/scratch/L0-trained-models-outputs/RD-L0-Modela/rd-L0-{noise}Nt-{N_s}Ns-lr-{lr}-L0-{lambda0}')\n",
    "        results.append((lr, hidden_dim, mean_error, ci95))\n",
    "\n",
    "        if mean_error < best_global_val_loss:\n",
    "            best_global_val_loss = mean_error\n",
    "            best_model_state = copy.deepcopy(net.state_dict())\n",
    "            best_hparams = {'lr': lr, 'hidden_dim': hidden_dim}\n",
    "\n",
    "# Final results\n",
    "#   for r in results:\n",
    "#       print(f\"lr={r[0]:.0e}, hidden_dim={r[1]:>3} → Val Error: {r[2]:.6f} ± {r[3]:.6f}\")\n",
    "\n",
    "print(f\"Best Hyperparameters: {best_hparams}, Validation Error: {best_global_val_loss:.6f}\")\n",
    "# torch.save(net,f'/home/esaha/links/scratch/L0-trained-models-outputs/RD-L0-Modela/rd-L0-best-SPINN-Datasize-compare-{N_s}Ns-{N_t}Nt-L0-{lambda0}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
