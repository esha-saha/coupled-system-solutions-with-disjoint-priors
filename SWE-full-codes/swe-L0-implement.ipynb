{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ff448",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "# from pyDOE2 import lhs\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from scipy import stats\n",
    "import time\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b859f7c",
   "metadata": {},
   "source": [
    "### Solve the system of shallow water equations to generate simulated data\n",
    "\n",
    "$ h_t + hu_x = 0$\n",
    "\n",
    "$ (hu)_t + \\left(hu^2 + \\frac{1}{2}gh^2\\right)_x + g(z_b)_x=0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d832cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "g = 9.81 #gravitational constant\n",
    "nx = 200 #number of spatial points\n",
    "L = 10.0 #Initial height if water\n",
    "dx = L / nx #spatial grid size\n",
    "dt = 0.001 #temporal grid size\n",
    "nt = 1000 #number of timesteps\n",
    "\n",
    "x = np.linspace(0, L, nx)\n",
    "\n",
    "# Solution array: (2, nx, nt)\n",
    "U = np.zeros((2, nx, nt))\n",
    "\n",
    "# Initial conditions\n",
    "h0 = np.ones(nx)\n",
    "h0[int(nx/2):] = 0.5\n",
    "u0 = np.zeros(nx)\n",
    "hu0 = h0 * u0\n",
    "\n",
    "U[0, :, 0] = h0\n",
    "U[1, :, 0] = hu0\n",
    "\n",
    "def flux(U_col):\n",
    "    h = U_col[0]\n",
    "    hu = U_col[1]\n",
    "    u = hu / h if h > 1e-6 else 0.0\n",
    "    return np.array([\n",
    "        hu,\n",
    "        hu * u + 0.5 * g * h**2\n",
    "    ])\n",
    "\n",
    "def rusanov_flux(U_left, U_right):\n",
    "    f_left = flux(U_left)\n",
    "    f_right = flux(U_right)\n",
    "\n",
    "    h_L = U_left[0]\n",
    "    h_R = U_right[0]\n",
    "    u_L = U_left[1] / h_L if h_L > 1e-6 else 0.0\n",
    "    u_R = U_right[1] / h_R if h_R > 1e-6 else 0.0\n",
    "    c_L = np.sqrt(g * h_L)\n",
    "    c_R = np.sqrt(g * h_R)\n",
    "\n",
    "    s_max = max(abs(u_L) + c_L, abs(u_R) + c_R)\n",
    "\n",
    "    return 0.5 * (f_left + f_right) - 0.5 * s_max * (U_right - U_left)\n",
    "\n",
    "# Time stepping\n",
    "for n in range(1, nt):\n",
    "    U_new = U[:, :, n - 1].copy()\n",
    "\n",
    "    for i in range(1, nx - 1):\n",
    "        U_L = U[:, i - 1, n - 1]\n",
    "        U_C = U[:, i, n - 1]\n",
    "        U_R = U[:, i + 1, n - 1]\n",
    "\n",
    "        F_plus = rusanov_flux(U_C, U_R)\n",
    "        F_minus = rusanov_flux(U_L, U_C)\n",
    "\n",
    "        U_new[:, i] -= dt / dx * (F_plus - F_minus)\n",
    "\n",
    "    # Simple zero-gradient BCs\n",
    "    U_new[:, 0] = U_new[:, 1]\n",
    "    U_new[:, -1] = U_new[:, -2]\n",
    "\n",
    "    U[:, :, n] = U_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d156f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_index = 0\n",
    "tstart=1\n",
    "h = U[0, :, tstart:]\n",
    "hu = U[1, :, tstart:]\n",
    "u = np.where(h > 1e-6, hu / h, 0.0)\n",
    "\n",
    "min_h = np.min(U[0,:,tstart:],axis = 0)\n",
    "max_h = np.max(U[0,:,tstart:],axis = 0)\n",
    "\n",
    "min_hu = np.min(U[1,:,tstart:],axis = 0)\n",
    "max_hu = np.max(U[1,:,tstart:],axis = 0)\n",
    "\n",
    "min_u = np.min(u,axis = 0)\n",
    "max_u = np.max(u,axis = 0)\n",
    "\n",
    "\n",
    "N_x = nx-1\n",
    "N_t = nt-tstart\n",
    "std_noise = 0.00 #standard deviation of noise if applicable (if using noisy dataset)\n",
    "var = 0\n",
    "h_clean_norm = (U[0,:,tstart:] - min_h)/(max_h - min_h) \n",
    "hu_clean_norm = (U[1,:,tstart:] - min_hu)/(max_hu - min_hu)\n",
    "h_noisy = U[0,:,tstart:] + np.random.normal(0,std_noise,((N_x+1),N_t))\n",
    "hu_noisy = U[1,:,tstart:]  + np.random.normal(0,std_noise,((N_x+1),N_t))\n",
    "h = (h_noisy - min_h)/(max_h - min_h) #+ np.random.normal(0,0.01,((N_x+1),N_t))\n",
    "hu = (hu_noisy - min_hu)/(max_hu - min_hu) #+ np.random.normal(0,0.01,((N_x+1),N_t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1c9aa",
   "metadata": {},
   "source": [
    "### Create the training and validation dataset\n",
    "We first select $N_s$ random spatial points (note that $N_s<<nx$) and $steps$ random temporal points ($steps\\leq nt$) for building the training+validation set. Further the training-validation split is done based on a 80-20 split. \n",
    "The test set consists of all the points in the entire spatio-temporal domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "u_data = h.reshape((N_x+1),N_t) #+ np.random.normal(0,0.01,((N_x+1),N_t))\n",
    "b_data = hu.reshape((N_x+1),N_t) #+ np.random.normal(0,0.01,((N_x+1),N_t))\n",
    "u0_data = h[:,0].reshape((N_x+1),1)\n",
    "b0_data = hu[:,0].reshape((N_x+1),1)\n",
    "\n",
    "t_bounds = np.linspace(0,1,N_t)\n",
    "x_bounds = np.linspace(0,L,N_x+1)/L #Normalizing L=10\n",
    "\n",
    "t_data = np.tile(t_bounds,((N_x+1),1))\n",
    "# print(t_data.shape)\n",
    "\n",
    "x_data = x_bounds.reshape(-1,1)\n",
    "x_data = np.tile(x_data, (1, N_t))\n",
    "\n",
    "# print(x_data.shape)\n",
    "N_s = 100\n",
    "steps = 800 #int(0.8*(N_t-std_noise))\n",
    "print('N_s', N_s, 'and Nt',steps)\n",
    "idx_s = np.random.choice(x_data.shape[0], N_s, replace = False)\n",
    "idx_t = np.random.choice(N_t,steps, replace = False)\n",
    "# print('index chosen for time is',idx_t)\n",
    "\n",
    "h_max = np.tile(max_h,(N_x+1)).squeeze().reshape((N_x+1),N_t)\n",
    "hu_max = np.tile(max_hu,(N_x+1)).squeeze().reshape((N_x+1),N_t)\n",
    "h_min = np.tile(min_h,(N_x+1)).squeeze().reshape((N_x+1),N_t)\n",
    "hu_min = np.tile(min_hu,(N_x+1)).squeeze().reshape((N_x+1),N_t)\n",
    "u_max = np.tile(max_u,(N_x+1)).squeeze().reshape((N_x+1),N_t)\n",
    "u_min = np.tile(min_u,(N_x+1)).squeeze().reshape((N_x+1),N_t)\n",
    "\n",
    "\n",
    "t_meas = t_data[idx_s, :]\n",
    "t_meas = t_meas[:, idx_t].reshape((-1,1))\n",
    "x_meas = x_data[idx_s, :]\n",
    "x_meas = x_meas[:, idx_t].reshape((-1,1))\n",
    "h_max_meas = h_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "h_min_meas = h_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "hu_max_meas = hu_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "hu_min_meas = hu_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "u_max_meas = u_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "u_min_meas = u_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "\n",
    "u_meas = u_data[idx_s, :]\n",
    "u_meas = u_meas[:, idx_t].reshape((-1,1))\n",
    "b_meas = b_data[idx_s, :]\n",
    "b_meas = b_meas[:, idx_t].reshape((-1,1))\n",
    "\n",
    "\n",
    "X_meas = np.hstack((x_meas, t_meas))\n",
    "\n",
    "Split_TrainVal = 0.8\n",
    "N_train = int(N_s*steps*Split_TrainVal)\n",
    "idx_train = np.random.choice(X_meas.shape[0], N_train, replace=False)\n",
    "# print(idx_train.shape)\n",
    "X_train = X_meas[idx_train,:]\n",
    "h_max_train = h_max_meas[idx_train,:]\n",
    "h_min_train = h_min_meas[idx_train,:]\n",
    "hu_max_train = hu_max_meas[idx_train,:]\n",
    "hu_min_train = hu_min_meas[idx_train,:]\n",
    "u_max_train = u_max_meas[idx_train,:]\n",
    "u_min_train = u_min_meas[idx_train,:]\n",
    "\n",
    "u_train = u_meas[idx_train,:]\n",
    "b_train = b_meas[idx_train,:]\n",
    "\n",
    "# Validation Measurements, which are the rest of measurements\n",
    "idx_val = np.setdiff1d(np.arange(X_meas.shape[0]), idx_train, assume_unique=True)\n",
    "X_val = X_meas[idx_val,:]\n",
    "h_max_val = h_max_meas[idx_val,:]\n",
    "h_min_val = h_min_meas[idx_val,:]\n",
    "hu_max_val = hu_max_meas[idx_val,:]\n",
    "hu_min_val = hu_min_meas[idx_val,:]\n",
    "u_max_val = u_max_meas[idx_val,:]\n",
    "u_min_val = u_min_meas[idx_val,:]\n",
    "\n",
    "u_val = u_meas[idx_val,:]\n",
    "b_val = b_meas[idx_val,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bddb2a",
   "metadata": {},
   "source": [
    "### Define all the relevant functions: $\\ell_0$ function, the neural network, count_weight function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df1715",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L0Gate(nn.Module):\n",
    "    def __init__(self, shape, droprate_init=0.5, temperature=2./3.):\n",
    "        super().__init__()\n",
    "        self.qz_loga = nn.Parameter(torch.Tensor(shape))\n",
    "        self.temperature = temperature\n",
    "        # init log-alpha\n",
    "        self.qz_loga.data.normal_(mean=np.log(droprate_init) - np.log(1 - droprate_init), std=1e-2)\n",
    "\n",
    "    def _hard_concrete_sample(self):\n",
    "        u = torch.rand_like(self.qz_loga)\n",
    "        s = torch.sigmoid((torch.log(u) - torch.log(1 - u) + self.qz_loga) / self.temperature)\n",
    "        z = s* (1.1 - 0.1) + 0.1  # Stretch to (0.1, 1.1)\n",
    "        return torch.clamp(z, 0, 1)\n",
    "\n",
    "    def forward(self):\n",
    "        return self._hard_concrete_sample()\n",
    "\n",
    "    def l0_loss(self):\n",
    "        # Expected gate value → expected L0 norm\n",
    "        s = torch.sigmoid(self.qz_loga)\n",
    "        return torch.sum(s)\n",
    "\n",
    "def count_weights_and_nnz(net):\n",
    "    total_weights = 0\n",
    "    total_nnz = 0\n",
    "\n",
    "    layers = [\n",
    "        (net.hidden_layer1, net.g1),\n",
    "        (net.hidden_layer2, net.g2),\n",
    "        (net.hidden_layer3, net.g3),\n",
    "        (net.hidden_layer4, net.g4),\n",
    "        #   (net.hidden_layer5, net.g5),\n",
    "        #   (net.hidden_layer6, net.g6),\n",
    "    ]\n",
    "\n",
    "    for layer, gate in layers:\n",
    "        w = layer.weight\n",
    "        b = layer.bias\n",
    "\n",
    "        in_features = w.shape[1]\n",
    "        out_features = w.shape[0]\n",
    "\n",
    "        # Count total weights in this layer\n",
    "        total_layer = w.numel() + b.numel()\n",
    "        total_weights += total_layer\n",
    "\n",
    "        # Expected active neurons from gate (soft L0)\n",
    "        s = torch.sigmoid(gate.qz_loga).detach()\n",
    "        active = (s > 0.5).float()  # hard threshold\n",
    "        n_active = int(active.sum().item())\n",
    "\n",
    "        # Each active neuron has all its incoming weights + 1 bias\n",
    "        nnz_layer = n_active * (in_features + 1)\n",
    "        total_nnz += nnz_layer\n",
    "\n",
    "    # Output layer (fully dense)\n",
    "    w = net.output_layer.weight\n",
    "    b = net.output_layer.bias\n",
    "    total_weights += w.numel() + b.numel()\n",
    "    total_nnz += w.numel() + b.numel()\n",
    "\n",
    "    return total_weights, total_nnz\n",
    "\n",
    "#Activation function\n",
    "def m(x):\n",
    "    return torch.sin(x)\n",
    "#   return torch.relu(x)\n",
    "\n",
    "def cart_inputs(x,t):\n",
    "    a = np.array([[x0,t0] for x0 in x for t0 in t])\n",
    "    return a[:,0].reshape(-1,1), a[:,1].reshape(-1,1)\n",
    "\n",
    "#Neural network; uncomment/comment based on how many layers you want in the network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, H):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden_layer1 = nn.Linear(2, H)\n",
    "        self.hidden_layer2 = nn.Linear(H, H)\n",
    "        self.hidden_layer3 = nn.Linear(H, H)\n",
    "        self.hidden_layer4 = nn.Linear(H, H)\n",
    "        # self.hidden_layer5 = nn.Linear(H, H)\n",
    "        # self.hidden_layer6 = nn.Linear(H, H)\n",
    "\n",
    "        # Add gates (one per neuron)\n",
    "        self.g1 = L0Gate((H,))\n",
    "        self.g2 = L0Gate((H,))\n",
    "        self.g3 = L0Gate((H,))\n",
    "        self.g4 = L0Gate((H,))\n",
    "        # self.g5 = L0Gate((H,))\n",
    "        # self.g6 = L0Gate((H,))\n",
    "\n",
    "        self.output_layer = nn.Linear(H, 2)\n",
    "\n",
    "    def forward(self, x,t,hmax,hmin,humax,humin,umax,umin):\n",
    "        inputs = torch.cat([x,t],axis=1)\n",
    "\n",
    "        z1 = self.g1()\n",
    "        z2 = self.g2()\n",
    "        z3 = self.g3()\n",
    "        z4 = self.g4()\n",
    "        # z5 = self.g5()\n",
    "        # z6 = self.g6()\n",
    "\n",
    "        h1 = m(self.hidden_layer1(inputs)) * z1\n",
    "        h2 = m(self.hidden_layer2(h1)) * z2\n",
    "        h3 = m(self.hidden_layer3(h2)) * z3\n",
    "        h4 = m(self.hidden_layer4(h3)) * z4\n",
    "        # h5 = m(self.hidden_layer5(h4)) * z5\n",
    "        # h6 = m(self.hidden_layer6(h5)) * z6\n",
    "        output = self.output_layer(h4)\n",
    "        h = output[:,0].reshape(-1,1)\n",
    "        u = output[:,1].reshape(-1,1)\n",
    "        hu = h*u\n",
    "\n",
    "        h_x = torch.autograd.grad(h.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "        h_xx = torch.autograd.grad(h_x.sum(), x , create_graph=True,allow_unused=True)[0]\n",
    "        h_t = torch.autograd.grad(h.sum(), t, create_graph=True,allow_unused=True)[0]\n",
    "        hu_t = torch.autograd.grad(hu.sum(), t, create_graph=True,allow_unused=True)[0]\n",
    "        hu_x = torch.autograd.grad(hu.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "        u_x = torch.autograd.grad(u.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "        c_h = hmax-hmin\n",
    "        c_hu = humax-humin\n",
    "        c_u = umax - umin\n",
    "\n",
    "        F_out = 0\n",
    "        pde_u = ((hmax - hmin)*h_t + (1/L)*(humax-humin)*hu_x).reshape(-1,1)\n",
    "        hat_u_x = ((c_h*h+hmin)*(c_hu*hu_x)*(1/L)-(c_hu*hu+humin)*(c_h*h_x)*(1/L))/((c_h*h+hmin)**2)\n",
    "        hat_u = (c_hu*hu + humin)/(c_h*h+hmin)\n",
    "        pde_b = (c_hu*hu_t + (c_hu*hu+humin)*hat_u_x + (c_hu*hu_x)*(1/L)*(hat_u) + 9.8*(c_h*h + hmin)*(c_h*h_x)*(1/L)).reshape(-1,1)\n",
    "\n",
    "        return h,hu,pde_u,pde_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debed1fb",
   "metadata": {},
   "source": [
    "### Create all the relevant input-output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ic,t_ic = cart_inputs(x_bounds,t_bounds[0]*np.ones((1)))\n",
    "u_ic = torch.tensor(u0_data).reshape(-1).reshape(-1,1).detach().numpy()\n",
    "b_ic = torch.tensor(b0_data).reshape(-1).reshape(-1,1).detach().numpy()\n",
    "\n",
    "x_collocation = X_train[:,0].reshape(-1,1) #np.random.uniform(low=x_min, high=x_max, size=(N_x+1,1))\n",
    "t_collocation = X_train[:,1].reshape(-1,1)\n",
    "\n",
    "pt_x_ic = Variable(torch.from_numpy(x_ic).float(), requires_grad=True).to(device)\n",
    "pt_t_ic = Variable(torch.from_numpy(t_ic).float(), requires_grad=True).to(device)\n",
    "\n",
    "all_zeros = np.zeros((X_train.shape[0],1))\n",
    "pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
    "\n",
    "\n",
    "pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
    "\n",
    "x_val = X_val[:,0].reshape(-1,1) #np.random.uniform(low=x_min, high=x_max, size=(N_x+1,1))\n",
    "t_val = X_val[:,1].reshape(-1,1)\n",
    "\n",
    "pt_x_val = Variable(torch.from_numpy(x_val).float(), requires_grad=True).to(device)\n",
    "pt_t_val = Variable(torch.from_numpy(t_val).float(), requires_grad=True).to(device)\n",
    "h_max_train_t = torch.tensor(h_max_train).float().to(device)\n",
    "h_min_train_t = torch.tensor(h_min_train).float().to(device)\n",
    "hu_max_train_t = torch.tensor(hu_max_train).float().to(device)\n",
    "hu_min_train_t = torch.tensor(hu_min_train).float().to(device)\n",
    "u_max_train_t = torch.tensor(u_max_train).float().to(device)\n",
    "u_min_train_t = torch.tensor(u_min_train).float().to(device)\n",
    "\n",
    "h_max_val_t = torch.from_numpy(h_max_val).float().to(device)\n",
    "h_min_val_t = torch.from_numpy(h_min_val).float().to(device)\n",
    "hu_max_val_t = torch.from_numpy(hu_max_val).float().to(device)\n",
    "hu_min_val_t = torch.from_numpy(hu_min_val).float().to(device)\n",
    "u_max_val_t = torch.from_numpy(u_max_val).float().to(device)\n",
    "u_min_val_t = torch.from_numpy(u_min_val).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d8bc6d",
   "metadata": {},
   "source": [
    "### Main training loop and validation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter sets\n",
    "learning_rates = [0.005] #learning rate\n",
    "hidden_dim = 50 #number of neurons per layer\n",
    "lam_0 = [1e-3,1e-6] #regularization parameter\n",
    "num_repeats = 3  # repeat training with different seeds\n",
    "patience = 5 \n",
    "epochs_no_improve = 0\n",
    "max_epochs = 10000\n",
    "validate_every = 2000\n",
    "\n",
    "\n",
    "best_global_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_hparams = {}\n",
    "\n",
    "results = []\n",
    "results_h = []\n",
    "results_hu = []\n",
    "\n",
    "mse_cost_function1 = torch.nn.MSELoss() # Mean squared error\n",
    "\n",
    "for lr in learning_rates:\n",
    "# for hidden_dim in hidden_dims:\n",
    "    for lambda0 in lam_0:\n",
    "        run_errors = []\n",
    "        run_error_h = []\n",
    "        run_error_hu = []\n",
    "\n",
    "        print(f\"\\n=== Training with lr={lr}, L0={lambda0} ===hidden dim={hidden_dim}====\")\n",
    "\n",
    "        for run in range(num_repeats):\n",
    "            print(f\"Run {run+1}/{num_repeats}\")\n",
    "            torch.manual_seed(run)   # different init each repeat\n",
    "            t_start_train = time.time()\n",
    "\n",
    "            # Reinitialize model each time\n",
    "            net = Net(hidden_dim).to(device)\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "            # ---- Training loop ----\n",
    "            for epoch in range(max_epochs):\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                out, bout, f_out, g_out = net(pt_x_collocation, pt_t_collocation,\n",
    "                                            h_max_train_t, h_min_train_t,\n",
    "                                            hu_max_train_t, hu_min_train_t,\n",
    "                                            u_max_train_t, u_min_train_t)\n",
    "\n",
    "                net_ic, net_bic, _, _ = net(pt_x_ic, pt_t_ic,\n",
    "                                            torch.tensor(h_max[:,0]).float().to(device),\n",
    "                                            torch.tensor(h_min[:,0]).float().to(device),\n",
    "                                            torch.tensor(hu_max[:,0]).float().to(device),\n",
    "                                            torch.tensor(hu_min[:,0]).float().to(device),\n",
    "                                            torch.tensor(u_max[:,0]).float().to(device),\n",
    "                                            torch.tensor(u_min[:,0]).float().to(device))\n",
    "\n",
    "                h_out = out.reshape(-1, 1)\n",
    "                hu_out = bout.reshape(-1, 1)\n",
    "\n",
    "                # Loss\n",
    "                mse_h = mse_cost_function1(f_out, pt_all_zeros)\n",
    "                mse_hu = mse_cost_function1(g_out, pt_all_zeros)\n",
    "                mse_huic = mse_cost_function1(net_bic.reshape(-1,1), torch.from_numpy(b_ic.reshape(-1,1)).float().to(device))\n",
    "\n",
    "                mse_hic = mse_cost_function1(net_ic.reshape(-1,1), torch.from_numpy(u_ic.reshape(-1,1)).float().to(device))\n",
    "                mse_hdata = mse_cost_function1(h_out, torch.tensor(u_train).float().to(device))\n",
    "                mse_hudata = mse_cost_function1(hu_out, torch.tensor(b_train).float().to(device))\n",
    "\n",
    "                # loss = mse_hudata + mse_h + mse_hic\n",
    "                L0_term = (net.g1.l0_loss() + net.g2.l0_loss()   + net.g3.l0_loss() + net.g4.l0_loss())\n",
    "                        #  + net.g5.l0_loss() + net.g6.l0_loss())\n",
    "\n",
    "\n",
    "                # loss = mse_hudata + mse_h + mse_hic + lambda0 * L0_term\n",
    "                loss = mse_hdata + mse_huic + mse_hu + lambda0 * L0_term\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # ---- Validation ----\n",
    "                if (epoch) % validate_every == 0:\n",
    "                    net.eval()\n",
    "                    # print(mse_hdata, mse_hu)\n",
    "                    # with torch.no_grad():\n",
    "                    u_val_learnt, b_val_learnt, _, _ = net(pt_x_val, pt_t_val,\n",
    "                                                            h_max_val_t, h_min_val_t,\n",
    "                                                            hu_max_val_t, hu_min_val_t,\n",
    "                                                            u_max_val_t, u_min_val_t)\n",
    "\n",
    "                    rel_hudata_val = torch.norm(b_val_learnt - torch.tensor(b_val).float().to(device)) / torch.norm(torch.tensor(b_val).float().to(device))\n",
    "                    rel_hdata_val = torch.norm(u_val_learnt - torch.tensor(u_val).float().to(device)) / torch.norm(torch.tensor(u_val).float().to(device))\n",
    "\n",
    "                    val_loss = 0.5*(rel_hudata_val + rel_hdata_val).item()\n",
    "                    print(f\"Epoch {epoch}: Val Loss={val_loss:.4f}\")\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                        epochs_no_improve = 0\n",
    "                    else:\n",
    "                        epochs_no_improve += 1\n",
    "\n",
    "                    if epochs_no_improve == patience:\n",
    "                        print(f\"Early stopping at epoch {epoch}\")\n",
    "                        break\n",
    "\n",
    "            # Restore best model weights\n",
    "            net.load_state_dict(best_model_wts)\n",
    "            t_end_train = time.time()\n",
    "\n",
    "            # ---- Compute error metrics on this run ----\n",
    "            t_start_eval = time.time()\n",
    "            x1, t1 = cart_inputs(x_bounds, t_data[0,:])\n",
    "            pt_x = torch.from_numpy(x1).float().to(device).requires_grad_(True)\n",
    "            pt_t = torch.from_numpy(t1).float().to(device).requires_grad_(True)\n",
    "\n",
    "            pt_u, pt_b, _, _ = net(pt_x, pt_t,\n",
    "                                torch.tensor(h_max).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(h_min).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(hu_max).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(hu_min).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(u_max).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(u_min).reshape(-1,1).float().to(device))\n",
    "            t_end_eval = time.time()\n",
    "\n",
    "            ms_u = pt_u.reshape(x_bounds.shape[0], t_data[0,:].shape[0]) #.detach().cpu().numpy()\n",
    "            ms_b = pt_b.reshape(x_bounds.shape[0], t_data[0,:].shape[0]) #.detach().cpu().numpy()\n",
    "            full_field_true = torch.vstack((torch.tensor(h_clean_norm),torch.tensor(hu_clean_norm))).to(device)\n",
    "            full_field_net = torch.vstack((ms_u,ms_b)).detach() #.detach()\n",
    "\n",
    "            error_uv = torch.zeros(N_t)\n",
    "            error_hu = torch.zeros(N_t)\n",
    "            error_h = torch.zeros(N_t)\n",
    "            for i in range(N_t):\n",
    "                error_uv[i] = torch.norm(full_field_true[:,i] - full_field_net[:,i])/torch.norm(full_field_true[:,i])\n",
    "                error_hu[i] = torch.norm(torch.tensor(hu_clean_norm[:,i]).to(device) - ms_b[:,i])/torch.norm(torch.tensor(hu_clean_norm[:,i]).to(device))\n",
    "                error_h[i] = torch.norm(torch.tensor(h_clean_norm[:,i]).to(device) - ms_u[:,i])/torch.norm(torch.tensor(h_clean_norm[:,i]).to(device))\n",
    "\n",
    "            print('\\nError uv',torch.mean(error_uv),' Error u',torch.mean(error_h),'Error hu:', torch.mean(error_hu),'\\n')\n",
    "\n",
    "\n",
    "            error_h_fin = error_h[:]\n",
    "            error_hu_fin = error_hu[:]\n",
    "            error_uv_fin = error_uv[:]\n",
    "\n",
    "            print('\\nError uv',torch.mean(error_uv_fin),' Error u',torch.mean(error_h_fin),'Error hu:', torch.mean(error_hu_fin),'\\n')\n",
    "\n",
    "            error_uv = error_uv_fin\n",
    "            error_h = error_h_fin\n",
    "            error_hu = error_hu_fin\n",
    "\n",
    "            final_error = torch.mean(error_uv)\n",
    "            run_errors.append(final_error.item())\n",
    "            run_error_h.append(error_h)\n",
    "            run_error_hu.append(error_hu)\n",
    "            # print()\n",
    "            print(f\"Run {run+1}/{num_repeats} -> error={final_error.item():.4f}\")\n",
    "            total_w, nnz_w = count_weights_and_nnz(net)\n",
    "            perc = 100 * nnz_w / total_w\n",
    "\n",
    "            print(f\"\\nTotal weights: {total_w}\")\n",
    "            print(f\"\\nNon-zero (active) weights: {nnz_w}\")\n",
    "            print(f\"\\nPercentage active: {perc:.2f}%\")\n",
    "            print(f\"\\nTotal training time is\",t_end_train-t_start_train)\n",
    "            print(f\"\\nTotal evaluation time is\",t_end_eval-t_start_eval)\n",
    "            print(\"\\n---------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "        # ---- Compute mean ± CI ----\n",
    "        mean_err = np.mean(run_errors)\n",
    "        sem = stats.sem(run_errors)\n",
    "        # print('run error',run_errors,sem)\n",
    "        ci95 = sem * stats.t.ppf((1+0.95)/2., len(run_errors)-1)\n",
    "\n",
    "        results.append((lr, hidden_dim, mean_err, ci95))\n",
    "        print(f\"===> lr={lr}, hidden_dim={hidden_dim}, L0={lambda0} and {N_t-std_noise} steps -> full error={mean_err:.4f} ± {ci95:.4f}\")\n",
    "\n",
    "        # Track global best\n",
    "        if mean_err < best_global_val_loss:\n",
    "            best_global_val_loss = mean_err\n",
    "            best_model_state = copy.deepcopy(net.state_dict())\n",
    "            best_hparams = {'lr': lr, 'hidden_dim': hidden_dim}\n",
    "\n",
    "print(f\"\\nBest hyperparameters: {best_hparams}, Error={best_global_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ca-pde-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
