{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e851b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "# import pandas as pd\n",
    "# from pyDOE2 import lhs\n",
    "from torch.autograd import Variable\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import time\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b6fa3",
   "metadata": {},
   "source": [
    "### Functions for solving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tridiagonal_matrix(m, n,diag_value, off_diag_above,off_diag_below):\n",
    "    # Create a square matrix of size n x n filled with zeros\n",
    "    matrix = np.zeros((m, n))\n",
    "\n",
    "    # Set the main diagonal\n",
    "    np.fill_diagonal(matrix, diag_value)\n",
    "\n",
    "    # Set the first diagonal below the main diagonal\n",
    "    np.fill_diagonal(matrix[1:], off_diag_below)\n",
    "\n",
    "    # Set the first diagonal above the main diagonal\n",
    "    np.fill_diagonal(matrix[:, 1:], off_diag_above)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "def hpc(x,upc):\n",
    "  if x>=upc:\n",
    "    out = 1\n",
    "  else:\n",
    "    out = 0\n",
    "  return out\n",
    "\n",
    "def func_f(u,b,upc,e,a):\n",
    "  f = np.zeros((u.shape[0],u.shape[1]))\n",
    "  for i in range(u.shape[0]):\n",
    "    for j in range(u.shape[1]):\n",
    "      # print(np.linalg.norm(np.exp(u[i,j])),np.linalg.norm(1+e*u[i,j]))\n",
    "      f[i,j] = hpc(u[i,j],upc)*b[i,j]*np.exp(u[i,j]/(1+e*u[i,j])) - a*u[i,j]\n",
    "  # print(np.min(u))\n",
    "  return f\n",
    "\n",
    "def func_g(u,b,upc,e,q):\n",
    "  g = np.zeros((u.shape[0],u.shape[1]))\n",
    "  for i in range(u.shape[0]):\n",
    "    for j in range(u.shape[1]):\n",
    "      g[i,j] = -hpc(u[i,j],upc)*(e/q)*b[i,j]*np.exp(u[i,j]/(1+e*u[i,j])) #+ np.random.normal(0,0.1,1)[0]\n",
    "  return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5232ae",
   "metadata": {},
   "source": [
    "### Solve the model to get simulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadff582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define x-y-t spatial and temporal bounds\n",
    "x_min = 0\n",
    "x_max = 10\n",
    "y_min = 0\n",
    "y_max = 10\n",
    "t_max = 10\n",
    "\n",
    "#Define total number of discrete points\n",
    "N_x = 50\n",
    "N_y = 50\n",
    "N_t = 100\n",
    "\n",
    "#define step sizes\n",
    "x_bounds = np.linspace(x_min,x_max,N_x+1)\n",
    "y_bounds = np.linspace(y_min,y_max,N_y+1)\n",
    "t_bounds = np.linspace(0,t_max,N_t)\n",
    "\n",
    "\n",
    "#define finite difference matrices\n",
    "D_x = tridiagonal_matrix(N_x+1,N_y+1, 0, -1,1)#to define d/dx\n",
    "D_x[:3,0] = np.array([-3,4,-1])\n",
    "D_x[N_x - 2:N_x+1,N_y] = np.array([1,-4,3])\n",
    "D_y = D_x.T#to define d/dy\n",
    "\n",
    "D_xx = tridiagonal_matrix(N_x+1,N_y+1, -2, 1,1)\n",
    "D_xx[:4,0] = np.array([2,-5,4,-1])\n",
    "D_xx[N_x - 3:N_x+1,N_y] = np.array([-1,4,-5,2])\n",
    "\n",
    "D_yy = D_xx.T\n",
    "U0 = np.loadtxt('/home/esaha/links/projects/def-wanghao/esaha/RD-Wildfire-system/U0.txt') #, sep=\" \", header=None)\n",
    "B0 = np.loadtxt('/home/esaha/links/projects/def-wanghao/esaha/RD-Wildfire-system/B0.txt')\n",
    "# B0 = pd.read_csv('/home/esaha/links/projects/def-wanghao/esaha/RD-Wildfire-system/B0.txt', sep=\" \", header=None)\n",
    "\n",
    "B0 = np.ones((N_x+1,N_y+1))\n",
    "print(x_bounds)\n",
    "for i in range(N_x+1):\n",
    "  for j in range(N_y+1):\n",
    "    B0[i,j] = 0.1*(x_bounds[i]/10 + y_bounds[j]/10+5) #0.5*np.exp((x_bounds[i]/10 - 0.5)**2 + (y_bounds[j]/10 - 0.5)**2)\n",
    "plt.imshow(B0)\n",
    "plt.colorbar()\n",
    "\n",
    "#Initialize Variables\n",
    "u = np.zeros((N_x+1,N_y+1,N_t))\n",
    "b = np.zeros((N_x+1,N_y+1,N_t))\n",
    "v1 = np.zeros((N_x+1,N_y+1,N_t))\n",
    "v2 = np.zeros((N_x+1,N_y+1,N_t))\n",
    "F = np.zeros((N_x+1,N_y+1,N_t))\n",
    "G = np.zeros((N_x+1,N_y+1,N_t))\n",
    "V = np.zeros((N_t,2,N_x+1,N_y+1))\n",
    "\n",
    "#Add in all initial values and parameters\n",
    "u[:,:,0] = U0[:N_x+1,:N_x+1] #+ 1.5*np.roll(U0.values[:N_x+1,:N_x+1],-15,axis = 0) + np.roll(U0.values[:N_x+1,:N_x+1],-18,axis = 1)\n",
    "b[:,:,0] = B0 #[:N_x+1,:N_x+1] #.values[:N_x+1,:N_x+1]\n",
    "VV = np.load('/home/esaha/links/projects/def-wanghao/esaha/RD-Wildfire-system/VV.npy') #+ np.random.normal(0,1,(101,2,128,128)) #size is timesteps * 2 * N_x+1 * N_y+1\n",
    "# V[:,0,:,:] = np.random.normal(0,0.5,(N_t,N_x+1,N_x+1))**2 #V[:N_t,:,:N_x+1,:N_y+1]\n",
    "# V[:,1,:,:] = np.random.normal(0,0.5,(N_t,N_x+1,N_x+1))**2\n",
    "V = VV[:N_t,:,:N_x+1,:N_y+1]\n",
    "alpha = 1e-3\n",
    "eps = 0.3\n",
    "q = 1\n",
    "kappa = 0.2\n",
    "upc = 3\n",
    "\n",
    "\n",
    "F[:,:,0] = func_f(u[:,:,0],b[:,:,0],upc,eps,alpha)\n",
    "G[:,:,0] = func_g(u[:,:,0],b[:,:,0],upc,eps,q)\n",
    "\n",
    "## Solving with boundary conditions\n",
    "# for i in range(1,N_t):\n",
    "#   # print(np.linalg.norm(np.exp(u[:,:,i])))\n",
    "#   F[1:-1,1:-1,i] = func_f(u[1:-1,1:-1,i-1],b[1:-1,1:-1,i-1],upc,eps,alpha)\n",
    "#   G[1:-1,1:-1,i] = func_g(u[1:-1,1:-1,i-1],b[1:-1,1:-1,i-1],upc,eps,q)\n",
    "#   u[1:-1,1:-1,i] = u[1:-1,1:-1,i-1] + (t_bounds[1]-t_bounds[0])*(kappa*(np.matmul(u[1:-1,1:-1,i-1],D_xx[1:-1,1:-1]) +\n",
    "#                                   np.matmul(D_yy[1:-1,1:-1],u[1:-1,1:-1,i-1])) -\n",
    "#                             (V[i-1,0,1:N_x,1:N_x]*(np.matmul(u[1:-1,1:-1,i-1],D_x[1:-1,1:-1])) + V[i-1,1,1:N_y,1:N_y]*(np.matmul(D_y[1:-1,1:-1],u[1:-1,1:-1,i-1]))) +\n",
    "#                             func_f(u[1:-1,1:-1,i-1],b[1:-1,1:-1,i-1],upc,eps,alpha))\n",
    "#   b[1:-1,1:-1,i] = b[1:-1,1:-1,i-1] + (t_bounds[1]-t_bounds[0])*func_g(u[1:-1,1:-1,i-1],b[1:-1,1:-1,i-1],upc,eps,q)\n",
    "#   if i%10==0:\n",
    "#     print('time step is',i)\n",
    "\n",
    "## Solving without boundary conditions\n",
    "for i in range(1,N_t):\n",
    "  # print(np.linalg.norm(np.exp(u[:,:,i])))\n",
    "  F[:,:,i] = func_f(u[:,:,i-1],b[:,:,i-1],upc,eps,alpha)\n",
    "  G[:,:,i] = func_g(u[:,:,i-1],b[:,:,i-1],upc,eps,q)\n",
    "  u[:,:,i] = u[:,:,i-1] + (t_bounds[1]-t_bounds[0])*(kappa*(np.matmul(u[:,:,i-1],D_xx) +\n",
    "                                  np.matmul(D_yy,u[:,:,i-1])) -\n",
    "                            (V[i-1,0,:N_x+1,:N_x+1]*(np.matmul(u[:,:,i-1],D_x)) + V[i-1,1,:N_y+1,:N_y+1]*(np.matmul(D_y,u[:,:,i-1]))) +\n",
    "                            func_f(u[:,:,i-1],b[:,:,i-1],upc,eps,alpha))\n",
    "  b[:,:,i] = b[:,:,i-1] + (t_bounds[1]-t_bounds[0])*func_g(u[:,:,i-1],b[:,:,i-1],upc,eps,q)\n",
    "  if i%10==0:\n",
    "    print('time step is',i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8753e6",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0503163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_bounds = np.linspace(0,1,80)\n",
    "x_bounds = np.linspace(0,1,N_x + 1)\n",
    "y_bounds = np.linspace(0,1,N_y + 1)\n",
    "data_u_norm = np.zeros((u.shape[0],u.shape[1],N_t))\n",
    "data_b_norm = np.zeros((b.shape[0],b.shape[1],N_t))\n",
    "data_v1_norm = np.zeros((N_t,V.shape[2],V.shape[3]))\n",
    "data_v2_norm = np.zeros((N_t,V.shape[2],V.shape[3]))\n",
    "u_min_vec = np.zeros(N_t)\n",
    "u_max_vec = np.zeros(N_t)\n",
    "b_min_vec = np.zeros(N_t)\n",
    "b_max_vec = np.zeros(N_t)\n",
    "v1_min_vec = np.zeros(N_t)\n",
    "v1_max_vec = np.zeros(N_t)\n",
    "v2_min_vec = np.zeros(N_t)\n",
    "v2_max_vec = np.zeros(N_t)\n",
    "\n",
    "for i in range(N_t):\n",
    "    u_min = np.min(u[:,:,i])\n",
    "    u_max = np.max(u[:,:,i])\n",
    "    data_u_norm[:,:,i] = (u[:,:,i] - u_min)/(u_max - u_min)\n",
    "\n",
    "    b_min = np.min(b[:,:,i])\n",
    "    b_max = np.max(b[:,:,i])\n",
    "    data_b_norm[:,:,i] = (b[:,:,i] - b_min)/(b_max - b_min)\n",
    "\n",
    "    v1_min = np.min(V[i,0,:,:])\n",
    "    v1_max = np.max(V[i,0,:,:])\n",
    "    if v1_min == v1_max:\n",
    "        data_v1_norm[i,:,:] = (V[i,0,:,:] - v1_min)\n",
    "    else:\n",
    "        data_v1_norm[i,:,:] = (V[i,0,:,:] - v1_min)/(v1_max - v1_min)\n",
    "    # print(V[i,0,0,:])\n",
    "\n",
    "    v2_min = np.min(V[i,1,:,:])\n",
    "    v2_max = np.max(V[i,1,:,:])\n",
    "    if v2_min==v2_max:\n",
    "        data_v2_norm[i,:,:] = (V[i,1,:,:] - v2_min)\n",
    "    else:\n",
    "        data_v2_norm[i,:,:] = (V[i,1,:,:] - v2_min)/(v2_max - v2_min)\n",
    "\n",
    "    u_min_vec[i] = u_min\n",
    "    u_max_vec[i] = u_max\n",
    "    b_min_vec[i] = b_min\n",
    "    b_max_vec[i] = b_max\n",
    "\n",
    "data_u = data_u_norm\n",
    "data_b = data_b_norm\n",
    "data_v1 = data_v1_norm\n",
    "data_v2 = data_v2_norm\n",
    "plt.plot(data_b_norm.reshape(-1))\n",
    "plt.imshow(data_b[:,:,30])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2717da",
   "metadata": {},
   "source": [
    "### Create the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3159d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = 20\n",
    "t2 = 100\n",
    "nt = int(t2-t1)\n",
    "u_data = data_u[:,:,t1:t2].reshape((N_x+1)*(N_y+1),nt)\n",
    "b_data = data_b[:,:,t1:t2].reshape((N_x+1)*(N_y+1),nt)\n",
    "u0_data = data_u[:,:,t1].reshape((N_x+1)*(N_y+1),1)\n",
    "b0_data = data_b[:,:,t1].reshape((N_x+1)*(N_y+1),1)\n",
    "v1_data = data_v1[t1:t2,:,:].reshape((N_x+1)*(N_y+1),nt)\n",
    "v2_data = data_v2[t1:t2,:,:].reshape((N_x+1)*(N_y+1),nt)\n",
    "F_data = F[:,:,t1:t2].reshape((N_x+1)*(N_y+1),nt)\n",
    "G_data = G[:,:,t1:t2].reshape((N_x+1)*(N_y+1),nt)\n",
    "\n",
    "t_data = t_bounds \n",
    "t_data = np.tile(t_data,((N_x+1)*(N_y+1),1))\n",
    "\n",
    "\n",
    "x_data = x_bounds.reshape(-1,1)\n",
    "x_data = np.tile(x_data, (1, N_x+1))\n",
    "x_data = np.reshape(x_data, (-1, 1))\n",
    "x_data = np.tile(x_data, (1, nt))\n",
    "\n",
    "y_data = y_bounds.reshape((1,-1)) #Note this reshape is (1,-1) and NOT (-1,1)\n",
    "y_data = np.tile(y_data, ((N_y+1), 1))\n",
    "y_data = np.reshape(y_data, (-1, 1))\n",
    "y_data = np.tile(y_data, (1, nt))\n",
    "\n",
    "# print(x_data,'\\n',y_data)\n",
    "N_s = 1000\n",
    "steps = 60\n",
    "print('N_s and N_t are:',N_s,N_t)\n",
    "idx_s = np.random.choice(x_data.shape[0], N_s, replace = False)\n",
    "idx_t = np.random.choice(nt,steps, replace = False)\n",
    "\n",
    "u_max = np.tile(u_max_vec[t1:t2],(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),nt)\n",
    "u_min = np.tile(u_min_vec[t1:t2],(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),nt)\n",
    "b_max = np.tile(b_max_vec[t1:t2],(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),nt)\n",
    "b_min = np.tile(b_min_vec[t1:t2],(((N_x+1)*(N_y+1)),1)).squeeze().reshape((N_x+1)*(N_y+1),nt)\n",
    "\n",
    "t_meas = t_data[idx_s, :]\n",
    "t_meas = t_meas[:, idx_t].reshape((-1,1))\n",
    "x_meas = x_data[idx_s, :]\n",
    "x_meas = x_meas[:, idx_t].reshape((-1,1))\n",
    "y_meas = y_data[idx_s, :]\n",
    "y_meas = y_meas[:, idx_t].reshape((-1,1))\n",
    "u_meas = u_data[idx_s, :]\n",
    "u_meas = u_meas[:, idx_t].reshape((-1,1))\n",
    "b_meas = b_data[idx_s, :]\n",
    "b_meas = b_meas[:, idx_t].reshape((-1,1))\n",
    "u_max_meas = u_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "u_min_meas = u_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "b_max_meas = b_max[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "b_min_meas = b_min[idx_s,:][:,idx_t].reshape((-1,1))\n",
    "\n",
    "v1_meas = v1_data[idx_s, :]\n",
    "v1_meas = v1_meas[:, idx_t].reshape((-1,1))\n",
    "v2_meas = v2_data[idx_s, :]\n",
    "v2_meas = v2_meas[:, idx_t].reshape((-1,1))\n",
    "F_meas = F_data[idx_s, :]\n",
    "F_meas = F_meas[:, idx_t].reshape((-1,1))\n",
    "G_meas = G_data[idx_s, :]\n",
    "G_meas = G_meas[:, idx_t].reshape((-1,1))\n",
    "\n",
    "X_meas = np.hstack((x_meas, y_meas, t_meas))\n",
    "\n",
    "\n",
    "Split_TrainVal = 0.8\n",
    "N_train = int(N_s*steps*Split_TrainVal)\n",
    "idx_train = np.random.choice(X_meas.shape[0], N_train, replace=False)\n",
    "X_train = X_meas[idx_train,:]\n",
    "u_train = u_meas[idx_train,:]\n",
    "b_train = b_meas[idx_train,:]\n",
    "v1_train = v1_meas[idx_train,:]\n",
    "v2_train = v2_meas[idx_train,:]\n",
    "F_train = F_meas[idx_train,:]\n",
    "G_train = G_meas[idx_train,:]\n",
    "u_max_train = u_max_meas[idx_train,:]\n",
    "u_min_train = u_min_meas[idx_train,:]\n",
    "b_max_train = b_max_meas[idx_train,:]\n",
    "b_min_train = b_min_meas[idx_train,:]\n",
    "# Validation Measurements, which are the rest of measurements\n",
    "idx_val = np.setdiff1d(np.arange(X_meas.shape[0]), idx_train, assume_unique=True)\n",
    "X_val = X_meas[idx_val,:]\n",
    "u_val = u_meas[idx_val,:]\n",
    "b_val = b_meas[idx_val,:]\n",
    "v1_val = v1_meas[idx_val,:]\n",
    "v2_val = v2_meas[idx_val,:]\n",
    "F_val = v2_meas[idx_val,:]\n",
    "u_max_val = u_max_meas[idx_val,:]\n",
    "u_min_val = u_min_meas[idx_val,:]\n",
    "b_max_val = b_max_meas[idx_val,:]\n",
    "b_min_val = b_min_meas[idx_val,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974d703",
   "metadata": {},
   "source": [
    "### Define relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec9f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m(x):\n",
    "    return torch.relu(x)\n",
    "\n",
    "def compute_residuals(u, b, x, y, t, umin, umax, bmin, bmax, v1,v2):\n",
    "    u_x = torch.autograd.grad(u.sum(), x, create_graph=True,allow_unused=True)[0]\n",
    "    u_xx = torch.autograd.grad(u_x.sum(), x , create_graph=True,allow_unused=True)[0]\n",
    "    u_y = torch.autograd.grad(u.sum(), y, create_graph=True,allow_unused=True)[0]\n",
    "    u_yy = torch.autograd.grad(u_y.sum(), y , create_graph=True,allow_unused=True)[0]\n",
    "    u_t = torch.autograd.grad(u.sum(), t, create_graph=True,allow_unused=True)[0]\n",
    "    b_t = torch.autograd.grad(b.sum(), t, create_graph=True,allow_unused=True)[0]\n",
    "    c1 = (umax - umin) #.reshape(-1)\n",
    "    c2 = (bmax - bmin)#.reshape(-1)\n",
    "    # print(c1.shape,c2.shape,u_t.shape,u.shape)\n",
    "    f = Variable(torch.from_numpy(func_f((c1*u + umin).detach().cpu().numpy().reshape(-1,1),\n",
    "                                            (c2*b + bmin).detach().cpu().numpy().reshape(-1,1),upc,eps,alpha)).float()\n",
    "                                            , requires_grad=True).to(device)\n",
    "    g = Variable(torch.from_numpy(func_g((c1*u + umin).detach().cpu().numpy().reshape(-1,1),\n",
    "                                            (c2*b + bmin).detach().cpu().numpy().reshape(-1,1),upc,eps,q)).float()\n",
    "                                            , requires_grad=True).to(device)\n",
    "\n",
    "    pde_u = -(c1/10)*u_t + kappa*(c1**2/100)*(u_yy + u_xx) - v1.reshape(-1,1)*(c1/10)*u_x - v2.reshape(-1,1)*(c1/10)*u_y  + (-g/0.3 - 0.001*c1*(u+umin).reshape(-1,1)) #(0.1*u_xx + 0.1*u_yy - u*v**2 - u**3 + v**3 +u**2*v + u - u_t).reshape(-1,1)\n",
    "    # pde_u = (0.1*(c_u/L**2)*u_xx + 0.1*(c_u/L**2)*u_yy - (c_u*u+umin)*(c_v*v+vmin)**2 - (c_u*u+umin)**3 + (c_v*v+vmin)**3 + (c_u*u+umin)**2*(c_v*v+vmin) + (c_u*u+umin) - (c_u/10)*u_t).reshape(-1,1)\n",
    "    pde_b = (- (c2/(8))*(b_t.reshape(-1,1)) + g.reshape(-1,1)).reshape(-1,1)\n",
    "    # print(c1.shape,c2.shape,u_t.shape,u.shape,pde_u.shape)\n",
    "    return pde_u,pde_b\n",
    "\n",
    "    \n",
    "def hard_threshold(model, sparsity):\n",
    "    \"\"\"\n",
    "    Zero-out smallest weights BUT keep their gradients working.\n",
    "    \"\"\"\n",
    "    all_weights = []\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            all_weights.append(param.data.view(-1))\n",
    "\n",
    "    # concatenate all\n",
    "    all_weights = torch.cat(all_weights)\n",
    "\n",
    "    # find threshold\n",
    "    k = int((1 - sparsity) * all_weights.numel())  # number to keep\n",
    "    if k < 1:\n",
    "        return\n",
    "\n",
    "    threshold = torch.topk(all_weights.abs(), k, largest=True).values.min()\n",
    "\n",
    "    # apply threshold\n",
    "    for param in model.parameters():\n",
    "        mask = (param.data.abs() >= threshold).float()\n",
    "        param.data *= mask\n",
    "\n",
    "def nnz_percentage(model):\n",
    "    nz = 0\n",
    "    total = 0\n",
    "    for p in model.parameters():\n",
    "        total += p.numel()\n",
    "        nz += (p.data != 0).sum().item()\n",
    "    return 100 * nz / total\n",
    "\n",
    "def cart_inputs(x,y,t):\n",
    "    a = np.array([[x0, y0,t0] for x0 in x for y0 in y for t0 in t])\n",
    "    return a[:,0].reshape(-1,1), a[:,1].reshape(-1,1), a[:,2].reshape(-1,1)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,H):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(3,H)\n",
    "        self.hidden_layer2 = nn.Linear(H,H)\n",
    "        self.hidden_layer3 = nn.Linear(H,H)\n",
    "        self.hidden_layer4 = nn.Linear(H,H)\n",
    "        # self.hidden_layer5 = nn.Linear(H,H)\n",
    "        # self.hidden_layer6 = nn.Linear(H,H)\n",
    "        self.output_layer = nn.Linear(H,2)\n",
    "        \n",
    "    def forward(self, x,y,t,b_in,v1,v2,umin,umax,bmin,bmax):\n",
    "        inputs = torch.cat([x,y,t],axis=1)\n",
    "        layer1_out = m(self.hidden_layer1(inputs))\n",
    "        layer2_out = m(self.hidden_layer2(layer1_out))\n",
    "        layer3_out = m(self.hidden_layer3(layer2_out))\n",
    "        layer4_out = m(self.hidden_layer4(layer3_out))\n",
    "        # layer5_out = m(self.hidden_layer5(layer4_out))\n",
    "        # layer6_out = m(self.hidden_layer6(layer5_out))\n",
    "        output = self.output_layer(layer4_out)\n",
    "        u = output[:,0].reshape(-1,1)\n",
    "        b = output[:,1].reshape(-1,1)\n",
    "\n",
    "        pde_u = 0 \n",
    "        pde_b = 0 \n",
    "        return u,b,pde_u,pde_b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371954c9",
   "metadata": {},
   "source": [
    "### Define the input-output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ic,y_ic,t_ic = cart_inputs(x_bounds,y_bounds,t_bounds[0]*np.ones((1)))\n",
    "u_ic = torch.tensor(data_u[:N_x+1,:N_x+1,t1]).reshape(-1).reshape(-1,1).float() #.detach().numpy() #U0.values[:N_x+1,:N_x+1].reshape(-1) #.repeat(N_t,1)\n",
    "b_ic = torch.tensor(data_b[:N_x+1,:N_x+1,t1]).reshape(-1).reshape(-1,1).float() #.detach().numpy() #B0.values[:N_x+1,:N_x+1].reshape(-1)\n",
    "\n",
    "pt_x_ic = Variable(torch.from_numpy(x_ic).float(), requires_grad=True).to(device)\n",
    "pt_y_ic = Variable(torch.from_numpy(y_ic).float(), requires_grad=True).to(device)\n",
    "pt_t_ic = Variable(torch.from_numpy(t_ic).float(), requires_grad=True).to(device)\n",
    "\n",
    "pt_u_ic = Variable(u_ic, requires_grad=True).to(device)\n",
    "pt_b_ic = Variable(b_ic, requires_grad=True).to(device)\n",
    "\n",
    "pt_V = Variable(torch.from_numpy(V[t1:t2,:,:,:]).float(), requires_grad=True).to(device)\n",
    "\n",
    "x_collocation = X_train[:,0].reshape(-1,1) #np.random.uniform(low=x_min, high=x_max, size=(N_x+1,1))\n",
    "y_collocation = X_train[:,1].reshape(-1,1) #np.random.uniform(low=y_min, high=y_max, size=(N_y+1,1))\n",
    "t_collocation = X_train[:,2].reshape(-1,1)\n",
    "\n",
    "all_zeros = np.zeros((X_train.shape[0],1))\n",
    "\n",
    "\n",
    "pt_x_collocation = Variable(torch.from_numpy(x_collocation).float(), requires_grad=True).to(device)\n",
    "pt_y_collocation = Variable(torch.from_numpy(y_collocation).float(), requires_grad=True).to(device)\n",
    "pt_t_collocation = Variable(torch.from_numpy(t_collocation).float(), requires_grad=True).to(device)\n",
    "v1_col = Variable(torch.from_numpy(v1_train).float(), requires_grad=True).to(device)\n",
    "v2_col = Variable(torch.from_numpy(v2_train).float(), requires_grad=True).to(device)\n",
    "pt_all_zeros = Variable(torch.from_numpy(all_zeros).float(), requires_grad=False).to(device)\n",
    "\n",
    "x_val = X_val[:,0].reshape(-1,1) #np.random.uniform(low=x_min, high=x_max, size=(N_x+1,1))\n",
    "y_val = X_val[:,1].reshape(-1,1) #np.random.uniform(low=y_min, high=y_max, size=(N_y+1,1))\n",
    "t_val = X_val[:,2].reshape(-1,1)\n",
    "\n",
    "pt_x_val = Variable(torch.from_numpy(x_val).float(), requires_grad=True).to(device)\n",
    "pt_y_val = Variable(torch.from_numpy(y_val).float(), requires_grad=True).to(device)\n",
    "pt_t_val = Variable(torch.from_numpy(t_val).float(), requires_grad=True).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de07f601",
   "metadata": {},
   "source": [
    "### Full training, validation and evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa222200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Hyperparameters ===\n",
    "learning_rates = [5e-3] #,1e-6,1e-5] #,1e-4]\n",
    "lr=5e-3\n",
    "hidden_dims = [50,100]\n",
    "# hidden_dim = 50\n",
    "lam_0 = [0.25,0.75]\n",
    "num_repeats = 3\n",
    "validate_every = 2000\n",
    "max_epochs = 20000\n",
    "patience = 5\n",
    "\n",
    "best_global_val_loss = float('inf')\n",
    "best_model_state = None\n",
    "best_hparams = {}\n",
    "mse_cost_function1 = torch.nn.MSELoss() # Mean squared error\n",
    "results = []\n",
    "\n",
    "# for lr in learning_rates:\n",
    "for hidden_dim in hidden_dims:\n",
    "    for lambda0 in lam_0:\n",
    "        run_errors = []\n",
    "\n",
    "        print(f\"\\n=== Training with lr={lr}, hidden_dim={hidden_dim} ===sparsity (zeros) = {lambda0}\")\n",
    "\n",
    "        for run in range(num_repeats):\n",
    "            print(f\"Run {run + 1}/{num_repeats}\")\n",
    "            torch.manual_seed(run)\n",
    "            \n",
    "\n",
    "            net = Net(hidden_dim).to(device)\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay = 0.0001)\n",
    "\n",
    "            best_val_loss = float('inf')\n",
    "            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "            t_start_train = time.time()\n",
    "\n",
    "            for epoch in range(0, max_epochs):\n",
    "                net.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                uout, bout, _, _ = net(pt_x_collocation,pt_y_collocation,pt_t_collocation,torch.tensor(b_train).float().to(device),\n",
    "                                torch.tensor(v1_train).float().to(device),torch.tensor(v2_train).float().to(device),torch.tensor(u_min_train).float().to(device),\n",
    "                                torch.tensor(u_max_train).float().to(device),torch.tensor(b_min_train).float().to(device),\n",
    "                                torch.tensor(b_max_train).float().to(device))\n",
    "                \n",
    "                f_out, g_out = compute_residuals(\n",
    "                    uout, bout,\n",
    "                    pt_x_collocation, pt_y_collocation, pt_t_collocation,\n",
    "                    torch.tensor(u_min_train).float().to(device),\n",
    "                    torch.tensor(u_max_train).float().to(device),\n",
    "                    torch.tensor(b_min_train).float().to(device),\n",
    "                    torch.tensor(b_max_train).float().to(device),\n",
    "                    torch.tensor(v1_train).float().to(device),torch.tensor(v2_train).float().to(device)\n",
    "                )\n",
    "\n",
    "                net_uic, net_bic, _, _ = net(pt_x_ic, pt_y_ic, pt_t_ic,pt_b_ic,\n",
    "                            pt_V[t1,0,:,:].reshape(-1),pt_V[t1,1,:,:].reshape(-1),torch.tensor(u_min[:,0]).float().to(device),torch.tensor(u_max[:,0]).float().to(device),\n",
    "                            torch.tensor(b_min[:,0]).float().to(device),torch.tensor(b_max[:,0]).float().to(device))\n",
    "\n",
    "                u_out = uout.reshape(-1, 1)\n",
    "                b_out = bout.reshape(-1, 1)\n",
    "\n",
    "                mse_u = mse_cost_function1(f_out, pt_all_zeros)\n",
    "                mse_b = mse_cost_function1(g_out, pt_all_zeros)\n",
    "                mse_uic = mse_cost_function1(net_uic.reshape(-1, 1), pt_u_ic)\n",
    "                mse_bic = mse_cost_function1(net_bic.reshape(-1, 1), pt_b_ic) #torch.from_numpy(u_ic.reshape(-1, 1)).float().to(device))\n",
    "                mse_udata = mse_cost_function1(u_out, torch.tensor(u_train).float().to(device))\n",
    "                mse_bdata = mse_cost_function1(b_out, torch.tensor(b_train).float().to(device))\n",
    "\n",
    "                # Weight the regularization\n",
    "                # lambda_l0 = 1e-5  # tune this\n",
    "\n",
    "                loss = mse_udata + mse_b + mse_bic \n",
    "                # loss = mse_bdata + mse_u + mse_uic\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if epoch % validate_every == 0 and epoch > 10000:\n",
    "                    hard_threshold(net, sparsity=lambda0)\n",
    "                    net.eval()\n",
    "                    if epoch%2000==0:\n",
    "                        u_outval,b_outval,_,_ = net(pt_x_val,pt_y_val,pt_t_val,torch.tensor(b_val).float().to(device),\n",
    "                                                    torch.tensor(v1_val).float().to(device),torch.tensor(v2_val).float().to(device),torch.tensor(u_min_val).float().to(device),\n",
    "                                                    torch.tensor(u_max_val).float().to(device),torch.tensor(b_min_val).float().to(device),\n",
    "                                                    torch.tensor(b_max_val).float().to(device))\n",
    "\n",
    "                        rel_u = torch.norm(torch.tensor(u_val).reshape(-1).to(device) - u_outval.reshape(-1)) / \\\n",
    "                                torch.norm(torch.tensor(u_val).reshape(-1).to(device))\n",
    "\n",
    "                        rel_b = torch.norm(torch.tensor(b_val).reshape(-1).to(device) - b_outval.reshape(-1)) / \\\n",
    "                                torch.norm(torch.tensor(b_val).reshape(-1).to(device))\n",
    "\n",
    "                        val_loss = 0.5 * (rel_u + rel_b)\n",
    "                        print('At epoch',epoch,':',rel_u.item(),rel_b.item())\n",
    "\n",
    "                        if val_loss.item() < best_val_loss - 1e-6:\n",
    "                            best_val_loss = val_loss.item()\n",
    "                            best_model_wts = copy.deepcopy(net.state_dict())\n",
    "                            epochs_no_improve = 0\n",
    "                        else:\n",
    "                            epochs_no_improve += 1\n",
    "\n",
    "                        if epochs_no_improve >= patience:\n",
    "                            print(f\"Early stopping at epoch {epoch}\")\n",
    "                            break\n",
    "\n",
    "            t_end_train = time.time()\n",
    "            # # Load best weights\n",
    "            net.load_state_dict(best_model_wts)\n",
    "\n",
    "            # Evaluate final validation score\n",
    "            s=1\n",
    "            xx= x_bounds[::s] #np.linspace(x_min,x_max,41)\n",
    "            yy= y_bounds[::s] #np.linspace(x_min,x_max,41)\n",
    "            tt= t_data[0,:] #np.linspace(0,10,100)\n",
    "            x1,y1,tt1 = cart_inputs(xx,yy,tt)\n",
    "            pt_x = Variable(torch.from_numpy(x1).float(), requires_grad=True).to(device)\n",
    "            pt_y = Variable(torch.from_numpy(y1).float(), requires_grad=True).to(device)\n",
    "            pt_t = Variable(torch.from_numpy(tt1).float(), requires_grad=True).to(device)\n",
    "            pt_u,pt_b,_,_ = net(pt_x,pt_y,pt_t,torch.tensor(b_data).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(v1_data).reshape(-1,1).to(device).float(),\n",
    "                                torch.tensor(v2_data).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(u_max[::s,:]).reshape(-1,1).float().to(device),torch.tensor(u_min[::s,:]).reshape(-1,1).float().to(device),\n",
    "                                torch.tensor(b_max[::s,:]).reshape(-1,1).float().to(device),torch.tensor(b_min[::s,:]).reshape(-1,1).float().to(device))\n",
    "            ms_u = pt_u.reshape(xx.shape[0],yy.shape[0],tt.shape[0])\n",
    "            ms_b = pt_b.reshape(xx.shape[0],yy.shape[0],tt.shape[0])\n",
    "            u_true = data_u[:,:,t1:t2]\n",
    "            b_true = data_b[:,:,t1:t2]\n",
    "\n",
    "            full_field_true = torch.hstack((torch.tensor(u_true),torch.tensor(b_true))).to(device)\n",
    "            full_field_net = torch.hstack((ms_u,ms_b)).detach()\n",
    "\n",
    "            error_uv = torch.zeros(nt)\n",
    "            error_u = torch.zeros(nt)\n",
    "            error_b = torch.zeros(nt)\n",
    "            for i in range(nt):\n",
    "                error_uv[i] = torch.norm(full_field_true[:,:,i] - full_field_net[:,:,i])/torch.norm(full_field_true[:,:,i])\n",
    "                error_u[i] = torch.norm(torch.tensor(u_true[:,:,i]).to(device) - ms_u[:,:,i])/torch.norm(torch.tensor(u_true[:,:,i]).to(device))\n",
    "                error_b[i] = torch.norm(torch.tensor(b_true[:,:,i]).to(device) - ms_b[:,:,i])/torch.norm(torch.tensor(b_true[:,:,i]).to(device))\n",
    "\n",
    "            print('\\nError uv',torch.mean(error_uv),' Error u',torch.mean(error_u),'Error b:', torch.mean(error_b),'\\n')\n",
    "\n",
    "            final_error = torch.mean(error_uv) #+ torch.mean(error_v))\n",
    "            run_errors.append(final_error.item())\n",
    "            print(\"\\nNNZ% =\", nnz_percentage(net))\n",
    "           \n",
    "            print(f\"Run {run+1}/{num_repeats} Final Val Error: {final_error.item():.6f}\")\n",
    "            print(f\"Total training time is\",t_end_train-t_start_train)\n",
    "\n",
    "        # Compute mean ± 95% CI\n",
    "        mean_error = np.mean(run_errors)\n",
    "        sem = stats.sem(run_errors)\n",
    "        ci95 = sem * stats.t.ppf((1 + 0.95) / 2., num_repeats - 1)\n",
    "\n",
    "        print(f\"At lr={lr}, hidden_dim={hidden_dim} mean full-field L2 Error: {mean_error:.6f} ± {ci95:.6f}\")\n",
    "        \n",
    "        torch.save(net,f'/home/esaha/links/scratch/L0-trained-models-outputs/Wildfire-L0-models/wildfire-L0-FixedWind-Ns-{N_s}-{lambda0}PercSparsity')\n",
    "       \n",
    "print(f\"Best Hyperparameters: {best_hparams}, Validation Error: {best_global_val_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
